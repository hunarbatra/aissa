{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LLM intervention + Activation patching tutorial\n",
    "### Wednesday October 15th\n",
    "\n",
    "**Welcome to the third lab for [AI Safety and Alignment](https://robots.ox.ac.uk/~fazl/aisaa/), MT 2025!**\n",
    "\n",
    "This lab is split into two parts:\n",
    "\n",
    "* 1Ô∏è‚É£ We'll first introduce the `TransformerLens` (TL) library, recall the basics of transformer-based large language models, and see how TL provides useful tools for inspecting intermediate activations, and intervening during the forward pass.\n",
    "    * During this introduction, we'll learn how to add activation hooks to ablate various components--a very rudimentary tool for mechanistic interpretability!\n",
    "* 2Ô∏è‚É£ Following this, we'll jump into an excellent (shortened) tutorial directly from [ARENA](https://arena-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification), which walks through how to use two additional tools for mechanistic interpretability of large language models:\n",
    "    * [**direct logit attribution**](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=disz2gTx-jooAcR0a5r8e7LZ)\n",
    "    * and [**activation patching**](https://arxiv.org/abs/2202.05262)\n",
    "    \n",
    "The lab will introduce a range of tools for mechanistic interpretability by way of exploring the so-called ['indirect object identification'](https://arxiv.org/abs/2211.00593) (an optional paper on the day 3 reading list) task to ground our exploration. We'll see more details about this task later (note: this notebook aims to be totally self-contained, and no prior familiarity with the task should be necessary)\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 6px solid #e58e26;\n",
    "  background: linear-gradient(90deg, #fff8f0 0%, #fff4e6 100%);\n",
    "  padding: 1rem 1.25rem;\n",
    "  margin: 1.5rem 0;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n",
    "\">\n",
    "  <h2 style=\"\n",
    "    color: #e58e26;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    letter-spacing: 0.5px;\n",
    "  \">Scope & focus</h2>\n",
    "  <p style=\"margin: 0; color: #2f3e46; line-height: 1.5;\">\n",
    "This lab is focused on one particular aspect of mechanistic interpretability: finding evidence for the importance of particular layers' activations for implementing algorithmic behaviour.\n",
    "\n",
    "There are many other areas of mechanistic interpretability we won't have time to cover. Other popular techniques include (some of which were discussed in the lecture):\n",
    "* **Learning interpretable components or features** inside a language model (SAEs / transcoders / crosscoders / sparse layers approximations)\n",
    "* **Steering models** with learned latent directions\n",
    "* **Monitoring activations** for harmful behaviour\n",
    "\n",
    "#### <span>*For all of the above, we need to be able to read or write to the activations at intermediate layer(s)!*</span>\n",
    "\n",
    "\n",
    "\n",
    "As a result, it's worth learning a bit about `TransformerLens` regardless of ones' interest in circuit discovery--especially if you're considering a mech interp-based project\n",
    "      \n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "    \n",
    "This lab attempts to assume almost no experience with transformers / LLMs, but inevitably glosses over a lot due to time. There are lots of important technical details (e.g., handling of layer norm / centering) that are abstracted away by TransformerLens, which can be explored in as much detail as desired (some of this is rather interesting, albeit a bit of a tangent!)\n",
    "\n",
    "<h3><span style='color:green'>‚ö†Ô∏è We recommend that you prioritize diving deeply into any part that piques your interest, rather than optimizing to finish the whole lab (there is a lot here!)</span></h3>\n",
    "\n",
    "---\n",
    "\n",
    "## Credits\n",
    "\n",
    "* ARENA Transformer Interpretability tutorial: [https://arena-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification](https://arena-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification), by [Callum McDougall](https://www.perfectlynormal.co.uk/)\n",
    "* The first part of this tutorial is inspired by various resources below. Please do check them out for more thorough introductions!\n",
    "    - [TransformerLens demo](https://transformerlensorg.github.io/TransformerLens/generated/demos/Main_Demo.html)\n",
    "    - [The ARENA tutorial 1.2](https://arena-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to `TransformerLens`\n",
    "---\n",
    "\n",
    "<img src=\"https://james-oldfield.github.io/ap-images/images/tf.png\" width=\"300\">\n",
    "\n",
    "> image credit: ChatGPT\n",
    "\n",
    "This first section will be a brief introduction to the TransformerLens library.\n",
    "\n",
    "We'll start very basic, and cover the following:\n",
    "\n",
    "## Content & Learning Objectives\n",
    "\n",
    "### 1Ô∏è‚É£ Loading & sampling from pre-trained language models\n",
    "\n",
    "We'll start by looking at how to find and load pre-trained models with TransformerLens, and how to sample from them for next-token prediction.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Know where to look to find available models\n",
    "> * Learn how to sample from the model\n",
    "\n",
    "### 2Ô∏è‚É£ Caching activations\n",
    "\n",
    "Next, we'll look at how to return the intermediate activations throughout the network during a forward pass. If we want to manipulate internal activations/mechanisms, we first need to know how to get them! TransformerLens makes this very easy.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Recall the basic components of a gpt2-style encoder-only transformer language model.\n",
    "> * Learn how to return the values of specific intermediate layers' activations during a forward pass.\n",
    "\n",
    "### 3Ô∏è‚É£ Intervening during a forward pass\n",
    "\n",
    "Beyond simply returning intermediate representations for monitoring, interpretability, feature analysis (etc), it's often useful to make **interventions** into the forward pass itself, to make predictable changes to the output!\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand how TransformerLens hooks relates to PyTorch hooks and model interventions\n",
    "> * Write your own model hooks with TransformerLens, for:\n",
    ">     * zero-ablation\n",
    ">     * tensor token manipulation at specific target points\n",
    "> * Reason about which categories of layers might be performing which kinds of tasks, and use this knowledge to bring target changes to the model outputs.\n",
    "> * Leave this section with a hypothesis about which kinds of layers might be most important in the IOI task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See all available models**\n",
    "\n",
    "There are a number of pre-trained models available with TransformerLens!\n",
    "\n",
    "You can see the latest list at: [https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html). There are various other useful pieces of information here, including the dimensionality of the residual stream, number of layers, etc.\n",
    "\n",
    "<!-- <img src=\"images/modelprop.png\" width=\"750\">-->\n",
    "<img src=\"https://james-oldfield.github.io/ap-images/images/modelprop.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the pre-trained gpt2-small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to note the specific tokenizer the pre-trained model uses (to turn input prompts into a tensor of 'token' indices fed to the transformer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the token id associated with the beginning of text token (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "TransformerLens' gpt2 model prepends a 'beginning of sequence' token to the input by default.\n",
    "\n",
    "We can see how the tokenizer breaks up the input prompt into substrings with `model.to_str_tokens`, and the corresponding ids with `model.to_tokens` (note the presence of token `50256` at the front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 31373,   995,     0]], device='cuda:0')\n",
      "----\n",
      "<|endoftext|> 50256\n",
      "hello 31373\n",
      " world 995\n",
      "! 0\n"
     ]
    }
   ],
   "source": [
    "# [bos_token] + content\n",
    "toks = model.to_tokens('hello world!')#, prepend_bos=1)\n",
    "strs = model.to_str_tokens('hello world!')\n",
    "\n",
    "print(toks)\n",
    "print('----')\n",
    "for t, s in zip(list(toks.squeeze().detach().cpu().numpy()), strs):\n",
    "    print(s, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampling from models\n",
    "\n",
    "Let's look next at how to sample from the models. One way to do this is with `model.generate(.)`.\n",
    "\n",
    "There are a number of optional arguments you can give if desired.\n",
    "\n",
    "üëâ `Shift + Tab` usually brings up the docstring (at least on my macbook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae8dffe02294bcb93f43405e5e35ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Ben is in the city of New York City, and he's been in the city for a while.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_prompt = 'Big Ben is in the city of'\n",
    "\n",
    "torch.manual_seed(5)\n",
    "output = model.generate(\n",
    "    example_prompt,\n",
    "    max_new_tokens=16,\n",
    "    do_sample=True,\n",
    "    temperature=0.0, # <- 0.0=always takes the top token (matches t.lens), \\infinity=uniform\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Note that gpt2-small is kind of crappy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TransformerLens` helper utils\n",
    "\n",
    "- `utils.test_prompt(.)` provides a handy wrapper with lots of extra information! We will use this for the first warmup tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Big', ' Ben', ' is', ' in', ' the', ' city', ' of']\n",
      "Tokenized answer: [' London']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.79</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.30</span><span style=\"font-weight: bold\">% Token: | London|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m12.79\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.30\u001b[0m\u001b[1m% Token: | London|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.39 Prob:  4.20% Token: | New|\n",
      "Top 1th token. Logit: 12.79 Prob:  2.30% Token: | London|\n",
      "Top 2th token. Logit: 12.68 Prob:  2.05% Token: | Los|\n",
      "Top 3th token. Logit: 12.55 Prob:  1.81% Token: | San|\n",
      "Top 4th token. Logit: 11.83 Prob:  0.88% Token: | Chicago|\n",
      "Top 5th token. Logit: 11.80 Prob:  0.85% Token: | L|\n",
      "Top 6th token. Logit: 11.78 Prob:  0.83% Token: | Baltimore|\n",
      "Top 7th token. Logit: 11.75 Prob:  0.81% Token: | Las|\n",
      "Top 8th token. Logit: 11.72 Prob:  0.78% Token: | Houston|\n",
      "Top 9th token. Logit: 11.68 Prob:  0.76% Token: | Seattle|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' London'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' London'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_answer = \" London\" # <- how likely (in relative position terms) is this choice of future token?\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching activations\n",
    "\n",
    "What if we want to collect the activations at a particular intermediate layer?\n",
    "\n",
    "For example, to train an SAE! Or perform monitoring of activations?\n",
    "\n",
    "Let's recall the basic building blocks of a decoder-only transformer large language model:\n",
    "\n",
    "<!-- <img src=\"./images/anthropic-architecture.png\" width=\"700\">-->\n",
    "<img src=\"https://james-oldfield.github.io/ap-images/images/anthropic-architecture.png\" width=\"700\">\n",
    "\n",
    "> Image credit: https://transformer-circuits.pub/2021/framework/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named hook points (and where to find them!)\n",
    "\n",
    "Below are the relevant `TransformerLens` hooks as they relate to the above gpt2-style transformer diagram* (note that this is not an exhaustive list):\n",
    "\n",
    "| Hook Point Name | Contents | Description |\n",
    "|-----------------|-------------|-------------|\n",
    "| `hook_embed` | $x_{0}$ | token embedding (before any positional embeddings) |\n",
    "| `resid_pre` | $x_{i}$ | residual stream representation, before block $i$ |\n",
    "| `attn_out` | $\\mathrm{Attn}(x_i)$ | After the first layer norm + self-attention layer |\n",
    "| `resid_mid` | $x_{i+1}=x_i + \\mathrm{Attn}(x_i)$ | Contents of residual stream, after the first layer norm + self-attention layer |\n",
    "| `mlp_out` | $\\mathrm{MLP}\\left(x_{i+1}\\right)$ | The output of the MLP layer applied to current representation |\n",
    "| `resid_post` | $x_{i+2} = x_{i+1} + \\mathrm{MLP}\\left(x_{i+1}\\right)$ | `resid_mid` + MLP(x), i.e., after adding everything back to resid. stream; state after block $i$  |\n",
    "\n",
    "\n",
    "---\n",
    "*According to [here](https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-full-updated.png) -- note that some models have slightly different ways of chaining the blocks (e.g. Pythia performs $\\mathrm{MLP}(x_{i})$ on the original input, rather than cumulative $\\mathrm{MLP}(x_{i+1})$ like above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at getting some of the intermediate activations at these various points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Breaking news! AGI confirmed.'\n",
    "tokens = model.to_tokens(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'Breaking', ' news', '!', ' AG', 'I', ' confirmed', '.']\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(model.to_str_tokens(prompt))\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(\n",
    "    tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 1\n",
    "cache[f'blocks.{layer}.hook_mlp_out'].shape\n",
    "#cache[f'blocks.{layer}.mlp.hook_pre'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "The `names_filter` argument can take a lambda function to filter for particular target layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.1.hook_resid_pre\n",
      "blocks.1.ln1.hook_scale\n",
      "blocks.1.ln1.hook_normalized\n",
      "blocks.1.attn.hook_q\n",
      "blocks.1.attn.hook_k\n",
      "blocks.1.attn.hook_v\n",
      "blocks.1.attn.hook_attn_scores\n",
      "blocks.1.attn.hook_pattern\n",
      "blocks.1.attn.hook_z\n",
      "blocks.1.hook_attn_out\n",
      "blocks.1.hook_resid_mid\n",
      "blocks.1.ln2.hook_scale\n",
      "blocks.1.ln2.hook_normalized\n",
      "blocks.1.mlp.hook_pre\n",
      "blocks.1.mlp.hook_post\n",
      "blocks.1.hook_mlp_out\n",
      "blocks.1.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "# cache only target layer\n",
    "# logits, cache = model.run_with_cache(tokens, names_filter=lambda x: 'hook_mlp_out' in x)\n",
    "logits, cache = model.run_with_cache(tokens, names_filter=lambda x: 'blocks.1.' in x)\n",
    "for k in cache.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interventions\n",
    "\n",
    "We've seen how to get intermediate activations in an LLM. How can we next *edit* the model at this specific point?\n",
    "\n",
    "**Aside (from neel nanda's TL demo)**: PyTorch hooks provide a similar ability to modify a model at intermediate points during its forward pass.\n",
    "However, the primary difference is that TransformerLens' hooks work on the model *activations*, rather than specific layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_loss = model(\n",
    "    tokens,\n",
    "    return_type=\"loss\" # <-- here we can see the effect on the loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass in functions that take [`activations`, `HookPoint`], and return the modified activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iden_loss = model.run_with_hooks(\n",
    "    tokens,\n",
    "    fwd_hooks=[(\"blocks.5.hook_resid_post\", lambda x, hook: x)],\n",
    "    return_type=\"loss\" # <-- here we can see the effect on the loss,\n",
    ")\n",
    "\n",
    "za_loss = model.run_with_hooks(\n",
    "    tokens,\n",
    "    fwd_hooks=[(\"blocks.5.hook_resid_post\", lambda x, hook: -x)],\n",
    "    return_type=\"loss\" # <-- here we can see the effect on the loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.bar(['Original CE loss', 'Identity', 'Intervention'], [original_loss.detach().cpu().numpy(), iden_loss.detach().cpu().numpy(), za_loss.detach().cpu().numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling after intervention\n",
    "\n",
    "We can use the `model.hooks()` context manager to sample from the model whilst applying pytorch hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85d85d061364bc2a65a8ea0e7f21955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- ORIGINAL OUTPUT --\n",
      "The capital of France is now home to the world's largest concentration of the world's largest concentration of the\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f47453001349f29117313323d7abaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- MODIFIED OUTPUT --\n",
      "The capital of France is-, ( M(-,‚Ä¶,,,,, (-,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def my_hook(x, hook):\n",
    "    return -1.0*x\n",
    "    \n",
    "prompt = 'The capital of France is'\n",
    "\n",
    "output = model.generate(prompt, max_new_tokens=16, temperature=0.0)\n",
    "print('-- ORIGINAL OUTPUT --')\n",
    "print(output)\n",
    "\n",
    "with model.hooks(fwd_hooks=[(\"blocks.5.hook_resid_post\", my_hook)]):\n",
    "    output = model.generate(prompt, max_new_tokens=16, temperature=0.0)\n",
    "print('-- MODIFIED OUTPUT --')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indirect object identification (IOI)\n",
    "\n",
    "This is the 'algorithmic' task of finding the indirect object token, in the sentence and predicting it as the next token.\n",
    "\n",
    "E.g. in the prompt `\"When John and Mary went to the shops, John gave a drink to\"`:\n",
    "* `John` is the subject\n",
    "* `Mary` is the indirect object.\n",
    "\n",
    "The task is predicting the IO `Mary` as the next token.\n",
    "\n",
    "At a high-level, we're interested in understanding if/when/where specific capabilities (or factual knowledge) is isolated to particular subcomponents in the network.\n",
    "During the lab, we're going to see tools (of increasing sophistication) to explore this!\n",
    "\n",
    "### Example\n",
    "\n",
    "We can see that gpt-2 correctly predicts `Mary` as the next token in the prompt `\"When John and Mary went to the shops, John gave a drink to\"`.\n",
    "\n",
    "So, evidently gpt2-small is capable of executing the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'When', ' John', ' and', ' Mary', ' went', ' to', ' the', ' shops', ',', ' John', ' gave', ' a', ' drink', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.81</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49.47</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.81\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m49.47\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.81 Prob: 49.47% Token: | Mary|\n",
      "Top 1th token. Logit: 16.89 Prob: 19.71% Token: | them|\n",
      "Top 2th token. Logit: 16.01 Prob:  8.18% Token: | the|\n",
      "Top 3th token. Logit: 15.02 Prob:  3.04% Token: | his|\n",
      "Top 4th token. Logit: 14.71 Prob:  2.23% Token: | John|\n",
      "Top 5th token. Logit: 14.71 Prob:  2.23% Token: | their|\n",
      "Top 6th token. Logit: 14.29 Prob:  1.46% Token: | a|\n",
      "Top 7th token. Logit: 14.07 Prob:  1.18% Token: | her|\n",
      "Top 8th token. Logit: 13.70 Prob:  0.81% Token: | everyone|\n",
      "Top 9th token. Logit: 13.40 Prob:  0.60% Token: | one|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"When John and Mary went to the shops, John gave a drink to\"\n",
    "\n",
    "utils.test_prompt(prompt, \" Mary\", model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The question grounding our exploration now becomes: can we localize this to specific blocks, layers, or attention heads?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Task 1]\n",
    "\n",
    "The first tasks are a gentle warmup.\n",
    "\n",
    "**NOTE**: the warmup steps are trivially easy with LLM autocomplete! I'd highly recommend turning it off for this stage\n",
    "\n",
    "This warmup has two goals: \n",
    "* To introduce useful utilities from TransformerLens for mechanistic interpretability, by way of intervening on specific layers to bring targeted changes to the output logits.\n",
    "* To introduce **zero ablation**: setting intermediate activations at specific layer(s) to zero and seeing the effect on the output. This is a crude, imperfect measure of how \"necessary\" specific activations are for the task.\n",
    "    * **note**: this is sometimes called a \"noising\" algorithm. We're starting with a working model, selectively removing activations, and seeing if it breaks the circuit.\n",
    "\n",
    "<!--<img src=\"./images/warmup.png\" width=\"300\">-->\n",
    "<img src=\"https://james-oldfield.github.io/ap-images/images/warmup.png\" width=\"300\">\n",
    "\n",
    "> image credit: ChatGPT\n",
    "\n",
    "\n",
    "We will be working with the same prompt in 'indirect object identification' format:\n",
    "\n",
    "##### `\"When John and Mary went to the shops, John gave a drink to\"`\n",
    "\n",
    "Here are a list of useful hooks that will be helpful for the first warmup task:\n",
    "\n",
    "```python\n",
    "hook_embed\n",
    "hook_pos_embed\n",
    "blocks.i.hook_resid_pre\n",
    "blocks.i.ln1.hook_scale\n",
    "blocks.i.ln1.hook_normalized\n",
    "blocks.i.attn.hook_q\n",
    "blocks.i.attn.hook_k\n",
    "blocks.i.attn.hook_v\n",
    "blocks.i.attn.hook_attn_scores\n",
    "blocks.i.attn.hook_pattern\n",
    "blocks.i.attn.hook_z\n",
    "blocks.i.hook_attn_out\n",
    "blocks.i.hook_resid_mid\n",
    "blocks.i.ln2.hook_scale\n",
    "blocks.i.ln2.hook_normalized\n",
    "blocks.i.mlp.hook_pre\n",
    "blocks.i.mlp.hook_post\n",
    "blocks.i.hook_mlp_out\n",
    "blocks.i.hook_resid_post\n",
    "ln_final.hook_scale\n",
    "ln_final.hook_normalized\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Make an intervention to a single layer that leads to gpt2 predicting a token other than `Mary` as the next token</span>\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "> Importance: üîµüîµüîµ‚ö™‚ö™\n",
    "> ```\n",
    "\n",
    "For this, you'll need to:\n",
    "- Identify an appropriate layer (using the above list for guidance). Consider the various intermediate states in the Transformer model. Which layers' activations might be maximally important for the IOI task?\n",
    "- Write an appropriate hook function -- how can we modify the activations at this layer to change the next token prediction? At this point, we just want to be maximally destructive, rather than bring about a targeted change.\n",
    "\n",
    "If you are stuck at any point, it's perhaps worth revisiting the diagram of the residual stream from Anthropic above--from this we can see an overview of how components read and write to the residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'When', ' John', ' and', ' Mary', ' went', ' to', ' the', ' shops', ',', ' John', ' gave', ' a', ' drink', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.81</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49.47</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.81\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m49.47\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.81 Prob: 49.47% Token: | Mary|\n",
      "Top 1th token. Logit: 16.89 Prob: 19.71% Token: | them|\n",
      "Top 2th token. Logit: 16.01 Prob:  8.18% Token: | the|\n",
      "Top 3th token. Logit: 15.02 Prob:  3.04% Token: | his|\n",
      "Top 4th token. Logit: 14.71 Prob:  2.23% Token: | John|\n",
      "Top 5th token. Logit: 14.71 Prob:  2.23% Token: | their|\n",
      "Top 6th token. Logit: 14.29 Prob:  1.46% Token: | a|\n",
      "Top 7th token. Logit: 14.07 Prob:  1.18% Token: | her|\n",
      "Top 8th token. Logit: 13.70 Prob:  0.81% Token: | everyone|\n",
      "Top 9th token. Logit: 13.40 Prob:  0.60% Token: | one|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def my_hook(x, hook):\n",
    "    # TODO: modify x to change its values in whatever way necessary\n",
    "    # note that currently this acts like an identity function -- we're simply returning the activations as-is\n",
    "    return x\n",
    "    \n",
    "with model.hooks(fwd_hooks=[\n",
    "    # TODO: define one appropriate layer to apply the intervention\n",
    "    #(\"blocks.HOOK_ID_HERE\", my_hook),\n",
    "]):\n",
    "    utils.test_prompt(prompt, \" Mary\", model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>[solution]</strong></summary>\n",
    "\n",
    "This is by no means the only solution. Here we add random noise to the later layers' residual stream, at all token positions!\n",
    "    \n",
    "```python\n",
    "def my_hook(x, hook):\n",
    "    return torch.randn_like(x)\n",
    "    \n",
    "with model.hooks(fwd_hooks=[\n",
    "    (\"blocks.7.hook_resid_post\", my_hook),\n",
    "]):\n",
    "    utils.test_prompt(prompt, \" Mary\", model, prepend_bos=True)\n",
    "```\n",
    "        \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Find **four** hook points at `blocks.{8..11}` whose activations can all be set to the zero vector (**\"zero ablated\"**) whilst preserving the IOI task capability for this prompt (i.e. `Mary` still comes next)\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥üî¥‚ö™‚ö™\n",
    "> Importance: üîµüîµüîµüîµ‚ö™\n",
    "> ```\n",
    "\n",
    "For this, you'll need to:\n",
    "- Identify appropriate layers, using the above list for guidance\n",
    "- Write an appropriate hook function to set the activations to zero\n",
    "    * Note that this gives us a very crude idea of the possible importance of each layer! As the notebook progresses through the ARENA tutorial, we'll see increasingly powerful ways of collecting such evidence.\n",
    "\n",
    "<details>\n",
    "    <summary>Hint 1</summary>\n",
    "    \n",
    "* Which categories of layers' activations are presumably *least* important for the IOI task? What kind of processing is relevant to the task? (factual recall, algorithmic reasoning, etc)\n",
    "* How is information moved around in the self-attention layers? What about in the MLP layers? Which of these two kinds of mechanisms is most likely to be relevant to the IOI task? Perhaps think about how *you* would go about performing the IOI task yourself, if you were the residual stream representation of the final `\"to\"` token.\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>Hint 2</summary>\n",
    "    If you are still unsure, try to manually ablate each in turn:\n",
    "    <ol>\n",
    "    <li>the specified layers' <strong>attention</strong>'s activations</li>\n",
    "    <li>the specified layers' <strong>MLP</strong>'s output activations</li>\n",
    "    </ol>\n",
    "    Does removing one category appear to be more destructive than another? It's sufficient to notice this, and move on!\n",
    "    \n",
    "</details>\n",
    "\n",
    "Zero ablation overwrites a particular layers' activations to the zero vector during the forward pass. This is a crude way of seeing something like \"how the network would have behaved, had that activation not taken on such a value?\". This is weak, circumstantial evidence of whether these activations--in isolations--were *necessary* for the task\n",
    "\n",
    "#### Optional: questions to think about\n",
    "\n",
    "What is one potential drawback of using a vector of zeros as the replacement value, when simulating such a counterfactual. **What values might be more appropriate**?\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Will the zero vector be \"in-distribution\"?\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://james-oldfield.github.io/ap-images/images/zero_ablate.png\" width=\"300\">\n",
    "\n",
    "> image credit: ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'When', ' John', ' and', ' Mary', ' went', ' to', ' the', ' shops', ',', ' John', ' gave', ' a', ' drink', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.81</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49.47</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.81\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m49.47\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.81 Prob: 49.47% Token: | Mary|\n",
      "Top 1th token. Logit: 16.89 Prob: 19.71% Token: | them|\n",
      "Top 2th token. Logit: 16.01 Prob:  8.18% Token: | the|\n",
      "Top 3th token. Logit: 15.02 Prob:  3.04% Token: | his|\n",
      "Top 4th token. Logit: 14.71 Prob:  2.23% Token: | John|\n",
      "Top 5th token. Logit: 14.71 Prob:  2.23% Token: | their|\n",
      "Top 6th token. Logit: 14.29 Prob:  1.46% Token: | a|\n",
      "Top 7th token. Logit: 14.07 Prob:  1.18% Token: | her|\n",
      "Top 8th token. Logit: 13.70 Prob:  0.81% Token: | everyone|\n",
      "Top 9th token. Logit: 13.40 Prob:  0.60% Token: | one|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def my_hook(x, hook):\n",
    "    # TODO: implement zero ablation (currently we are again using the identity function)\n",
    "    return x\n",
    "\n",
    "# TODO: define appropriate hook points at the 4 layers\n",
    "with model.hooks(fwd_hooks=[\n",
    "    # (\"blocks.8.HOOK_POINT\", my_hook),\n",
    "    # (\"blocks.9.HOOK_POINT\", my_hook),\n",
    "    # (\"blocks.10.HOOK_POINT\", my_hook),\n",
    "    # (\"blocks.11.HOOK_POINT\", my_hook),\n",
    "]):\n",
    "    utils.test_prompt(prompt, \" Mary\", model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>[solution]</strong></summary>\n",
    "\n",
    "Zero-ablating the MLPs' outputs at these layers leaves the capability largely untouched! Why might this be?\n",
    "    \n",
    "```python\n",
    "def my_hook(x, hook):\n",
    "    # TODO: perform zero ablation\n",
    "    return torch.zeros_like(x)\n",
    "\n",
    "with model.hooks(fwd_hooks=[\n",
    "    (\"blocks.8.hook_mlp_out\", my_hook),\n",
    "    (\"blocks.9.hook_mlp_out\", my_hook),\n",
    "    (\"blocks.10.hook_mlp_out\", my_hook),\n",
    "    (\"blocks.11.hook_mlp_out\", my_hook),\n",
    "]):\n",
    "    utils.test_prompt(prompt, \" Mary\", model, prepend_bos=True)\n",
    "```\n",
    "        \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Define one forward hook (and appropriate hook point) that will cause the model to output the subject `John` as the next token instead</span>\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥üî¥üî¥‚ö™\n",
    "> Importance: üîµ‚ö™‚ö™‚ö™‚ö™\n",
    "> ```\n",
    "\n",
    "**Note**: this task is not so much related to the IOI task itself. However, it should be a useful exercise in reasoning about different layers' activations and their influence at various points in the network.\n",
    "\n",
    "It could be useful to recall the available hook points (for `i={0,...,n_layers}`):\n",
    "<details>\n",
    "<summary>See available hooks</summary>\n",
    "<code>hook_embed\n",
    "hook_pos_embed\n",
    "blocks.i.hook_resid_pre\n",
    "blocks.i.ln1.hook_scale\n",
    "blocks.i.ln1.hook_normalized\n",
    "blocks.i.attn.hook_q\n",
    "blocks.i.attn.hook_k\n",
    "blocks.i.attn.hook_v\n",
    "blocks.i.attn.hook_attn_scores\n",
    "blocks.i.attn.hook_pattern\n",
    "blocks.i.attn.hook_z\n",
    "blocks.i.hook_attn_out\n",
    "blocks.i.hook_resid_mid\n",
    "blocks.i.ln2.hook_scale\n",
    "blocks.i.ln2.hook_normalized\n",
    "blocks.i.mlp.hook_pre\n",
    "blocks.i.mlp.hook_post\n",
    "blocks.i.hook_mlp_out\n",
    "blocks.i.hook_resid_post\n",
    "ln_final.hook_scale\n",
    "ln_final.hook_normalized</code>\n",
    "</details>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<details>\n",
    "    <summary>Hint 1</summary>\n",
    "    <ul>\n",
    "    <li>At what point in the network do we have direct correspondence between the token dimensions and the original sequence of tokens, with their order preserved?</li>\n",
    "    <li>It might be helpful here to recall the shape of the activations `[batch_size, n_tokens, dimension]` --  we can modify this tensor in any way we want</li>\n",
    "    </ul>\n",
    "</details>\n",
    "<details>\n",
    "    <summary>Hint 2</summary>\n",
    "    <ul>\n",
    "    <li>What about super early in the network? How can we swap activations around before any kind of cross-token interactions / positional information?</li>\n",
    "    </ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'When', ' John', ' and', ' Mary', ' went', ' to', ' the', ' shops', ',', ' John', ' gave', ' a', ' drink', ' to']\n",
      "Tokenized answer: [' John']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.71</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.23</span><span style=\"font-weight: bold\">% Token: | John|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.71\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.23\u001b[0m\u001b[1m% Token: | John|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.81 Prob: 49.47% Token: | Mary|\n",
      "Top 1th token. Logit: 16.89 Prob: 19.71% Token: | them|\n",
      "Top 2th token. Logit: 16.01 Prob:  8.18% Token: | the|\n",
      "Top 3th token. Logit: 15.02 Prob:  3.04% Token: | his|\n",
      "Top 4th token. Logit: 14.71 Prob:  2.23% Token: | John|\n",
      "Top 5th token. Logit: 14.71 Prob:  2.23% Token: | their|\n",
      "Top 6th token. Logit: 14.29 Prob:  1.46% Token: | a|\n",
      "Top 7th token. Logit: 14.07 Prob:  1.18% Token: | her|\n",
      "Top 8th token. Logit: 13.70 Prob:  0.81% Token: | everyone|\n",
      "Top 9th token. Logit: 13.40 Prob:  0.60% Token: | one|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' John'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' John'\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def my_hook(x, hook):\n",
    "    # TODO: manipulate the tensor x as appropriate\n",
    "    return x\n",
    "    \n",
    "with model.hooks(fwd_hooks=[\n",
    "    # TODO: define an appropriate hook point\n",
    "    #(\"hook_HOOKPOINT\", my_hook),\n",
    "]):\n",
    "    utils.test_prompt(prompt, \" John\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><strong>[solution]</strong></summary>\n",
    "\n",
    "We can achieve this by swapping the pre-positional token embeddings:\n",
    "    \n",
    "```python\n",
    "def my_hook(x, hook):\n",
    "    # put the \"john\" token embedding into the subject position\n",
    "    x[:, 10, :] = x[:, 4, :]\n",
    "    return x\n",
    "    \n",
    "with model.hooks(fwd_hooks=[\n",
    "    (\"hook_embed\", my_hook),\n",
    "]):\n",
    "    utils.test_prompt(prompt, \" John\", model)\n",
    "\n",
    "# NOTE we can see this equivalent to modifying the input prompt to use the previous IO as new subject: \n",
    "with model.hooks(fwd_hooks=[]):\n",
    "    prompt2 = 'When John and Mary went to the shops, Mary gave a drink to'\n",
    "    utils.test_prompt(prompt2, \" John\", model)\n",
    "```\n",
    "        \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Activation patching (in pairs, or solo)\n",
    "\n",
    "So far, we've learned a little about a lot of the tools provided by `TransformerLens`, and seen one concrete tool for testing the importance of layers' activations--zero ablation--in exercise 2.\n",
    "\n",
    "#### As the first section closes, it's worth reflecting a little on the following in turn:\n",
    "\n",
    "1. Zero ablation is a crude test of the necessity of the activations for the IOI task (why?)\n",
    "    * Caveat: the zero vector will be out-of-distribution, and may risk destroying the ability of the model to perform the task for this (unrelated) reason alone!\n",
    "4. What might a test for sufficiency for a \"layers' activations' importance on the IOI task\" look like? What properties might it have, in terms of changes to model outputs?\n",
    "    * Hint: when using zero ablation to probe for the importance of activations, the process is: \"start with the model producing the *correct* output, then modify the activations such that the output is *incorrect*\" -- can we re-order this setup to lead to the above?\n",
    "\n",
    "We're going to see some partial answers to the latter question next through **activation patching** for the IOI task--a kind of \"denoising\" process\n",
    "\n",
    "From here on out, we're going to be using the excellent ARENA tutorials! You can find out more about ARENA below:\n",
    "\n",
    "[https://arena-chapter1-transformer-interp.streamlit.app/](https://arena-chapter1-transformer-interp.streamlit.app/)\n",
    "\n",
    "The rest of this notebook is almost entirely from the above tutorial, verbatim. Although we've removed some of the later parts we're not going to be looking at given time constraints, and removed some text which references findings in earlier ARENA tutorials in an attempt to make this self-contained. We've also removed the suggestion for amount of time to spend on each task, given we've speed-ran a lot of the basics. Note that the comments are written in first person from the POV of the original ARENA tutorials' author(s)!\n",
    "\n",
    "## Suggestions on how to approach the notebook\n",
    "\n",
    "To restate, we again encourage you to bear the following two things in mind:\n",
    "1. Optimize for depth over breadth -- there is far too much content here to digest meaningfully in 2-3 hours.\n",
    "2. The purpose of this tutorial is to get hands on experience with using `TransformerLens` for making interventions, and seeing which techniques provide us with what kinds of evidence.\n",
    "    * For today, we are not so concerned about learning the intricacies and details of the particular components and types of attention heads responsible for the IOI task--this is primarily a task to ground our exploration.\n",
    "\n",
    "Where we feel the need to add our own additional comments, we have added the following boxes throughout:\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 6px solid #2e8b57;\n",
    "  background: linear-gradient(90deg, #f6fff8 0%, #f0fff4 100%);\n",
    "  padding: 1rem 1.25rem;\n",
    "  margin: 1.5rem 0;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n",
    "\">\n",
    "  <h2 style=\"\n",
    "    color: #2e8b57;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    letter-spacing: 0.5px;\n",
    "  \">AISAA Note</h2>\n",
    "  <p style=\"\n",
    "    margin: 0;\n",
    "    color: #2f3e46;\n",
    "    line-height: 1.5;\n",
    "  \">\n",
    "    [here contains any useful additional information relevant to the lab session!]\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "With that noted, let's jump in to the (abridged) [ARENA](https://arena-chapter1-transformer-interp.streamlit.app/) tutorial below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rzWLrCntAgh"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-14-1.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6S4UXgYtAgh"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89hOoIcbtAgh"
   },
   "source": [
    "This notebook / document is built around the [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper, in which the authors aim to understand the **indirect object identification circuit** in GPT-2 small. This circuit is resposible for the model's ability to complete sentences like `\"John and Mary went to the shops, John gave a bag to\"` with the correct token \"`\" Mary\"`.\n",
    "\n",
    "It is loosely divided into different sections, each one with their own flavour. Sections 1, 2 & 3 are derived from Neel Nanda's notebook [Exploratory_Analysis_Demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=WXktSe0CvBdh). The flavour of these exercises is experimental and loose, with a focus on demonstrating what exploratory analysis looks like in practice with the transformerlens library. They skimp on rigour, and instead try to speedrun the process of finding suggestive evidence for this circuit. The code and exercises are simple and generic, but accompanied with a lot of detail about what each stage is doing, and why (plus several optional details and tangents).\n",
    "\n",
    "*Note - if you find yourself getting frequent CUDA memory errors, you can periodically call `torch.cuda.empty_cache()` to [free up some memory](https://stackoverflow.com/questions/57858433/how-to-clear-gpu-memory-after-pytorch-model-training-without-restarting-kernel).*\n",
    "\n",
    "Each exercise will have a difficulty and importance rating out of 5, and sometimes a short annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Moa6ABkatAgi"
   },
   "source": [
    "## The purpose / structure of these exercises\n",
    "\n",
    "At a surface level, these exercises are designed to take you through the indirect object identification circuit. But it's also designed to make you a better interpretability researcher! As a result, most exercises will be doing a combination of:\n",
    "\n",
    "1. Showing you some new feature/component of the circuit, and\n",
    "2. Teaching you how to use tools and interpret results in a broader mech interp context.\n",
    "\n",
    "A key idea to have in mind during these exercises is the **spectrum from simpler, more exploratory tools to more rigoruous, complex tools**. On the simpler side, you have something like inspecting attention patterns, which can give a decent (but sometimes misleading) picture of what an attention head is doing. These should be some of the first tools you reach for, and you should be using them a lot even before you have concrete hypotheses about a circuit. On the more rigorous side, you have something like path patching, which is a pretty rigorous and effortful tool that is best used when you already have reasonably concrete hypotheses about a circuit (note: which we won't be covering in the AISAA lab!). As we go through the exercises, we'll transition from left to right along this spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbLzBVUJtAgi"
   },
   "source": [
    "## The IOI task\n",
    "\n",
    "The first step when trying to reverse engineer a circuit in a model is to identify *what* capability we want to reverse engineer. Indirect Object Identification is a task studied in Redwood Research's excellent [Interpretability in the Wild](https://arxiv.org/abs/2211.00593) paper (see [Neel Nanda's interview with the authors](https://www.youtube.com/watch?v=gzwj0jWbvbo) or [Kevin Wang's Twitter thread](https://threadreaderapp.com/thread/1587601532639494146.html) for an overview). The task is to complete sentences like \"When Mary and John went to the store, John gave a drink to\" with \" Mary\" rather than \" John\".\n",
    "\n",
    "In the paper they rigorously reverse engineer a 26 head circuit, with 7 separate categories of heads used to perform this capability. The circuit they found roughly breaks down into three parts:\n",
    "\n",
    "1. Identify what names are in the sentence\n",
    "2. Identify which names are duplicated\n",
    "3. Predict the name that is *not* duplicated\n",
    "\n",
    "Why was this task chosen? The authors give a very good explanation for their choice in their [video walkthrough of their paper](https://www.youtube.com/watch?v=gzwj0jWbvbo), which you are encouraged to watch. To be brief, some of the reasons were:\n",
    "\n",
    "* This is a fairly common grammatical structure, so we should expect the model to build some circuitry for solving it quite early on (after it's finished with all the more basic stuff, like n-grams, punctuation, induction, and simpler grammatical structures than this one).\n",
    "* It's easy to measure: the model always puts a much higher probability on the IO and S tokens (i.e. `\" Mary\"` and `\" John\"`) than any others, and this is especially true once the model starts being stripped down to the core part of the circuit we're studying. So we can just take the logit difference between these two tokens, and use this as a metric for how well the model can solve the task.\n",
    "* It is a crisp and well-defined task, so less likely to be solved in terms of memorisation of a large bag of heuristics (unlike e.g. tasks like \"predict that the number `n+1` will follow `n`, which as Neel mentions in the video walkthrough is actually much more annoying and subtle than it first seems!).\n",
    "\n",
    "A terminology note: `IO` will refer to the indirect object (in the example, `\" Mary\"`), `S1` and `S2` will refer to the two instances of the subject token (i.e. `\" John\"`), and `end` will refer to the end token `\" to\"` (because this is the position we take our prediction from, and we don't care about any tokens after this point). We will also sometimes use `S` to refer to the identity of the subject token (rather than referring to the first or second instance in particular)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choLOG6OtAgi"
   },
   "source": [
    "## Keeping track of your guesses & predictions\n",
    "\n",
    "There's a lot to keep track of in these exercises as we work through them. You'll be exposed to new functions and modules from transformerlens, new ways to causally intervene in models, all the while building up your understanding of how the IOI task is performed. The notebook starts off exploratory in nature (lots of plotting and investigation), and gradually moves into more technical details, refined analysis, and replication of the paper's results, as we improve our understanding of the IOI circuit. You are recommended to keep a document or page of notes nearby as you go through these exercises, so you can keep track of the main takeaways from each section, as well as your hypotheses for how the model performs the task, and your ideas for how you might go off and test these hypotheses on your own if the notebook were to suddenly end.\n",
    "\n",
    "If you are feeling extremely confused at any point, you can come back to the dropdown below, which contains diagrams explaining how the circuit works. There is also an accompanying intuitive explanation which you might find more helpful. However, I'd recommend you try and go through the notebook unassisted before looking at these.\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 6px solid #2e8b57;\n",
    "  background: linear-gradient(90deg, #f6fff8 0%, #f0fff4 100%);\n",
    "  padding: 1rem 1.25rem;\n",
    "  margin: 1.5rem 0;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n",
    "\">\n",
    "  <h2 style=\"\n",
    "    color: #2e8b57;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    letter-spacing: 0.5px;\n",
    "  \">AISAA Note</h2>\n",
    "  <p style=\"\n",
    "    margin: 0;\n",
    "    color: #2f3e46;\n",
    "    line-height: 1.5;\n",
    "  \">\n",
    "  We strongly suggest you do not look at the additional explanations in the dropdowns directly below until right at the end (if you are particularly interested in the IOI circuit itself) -- this threatens to be an information overload in our limited time!\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Intuitive explanation of IOI circuit</summary>\n",
    "\n",
    "First, let's start with an analogy for how transformers work (you can skip this if you've already read [my post](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)). Imagine a line of people, who can only look forward. Each person has a token written on their chest, and their goal is to figure out what token the person in front of them is holding. Each person is allowed to pass a question backwards along the line (not forwards), and anyone can choose to reply to that question by passing information forwards to the person who asked. In this case, the sentence is `\"When Mary and John went to the store, John gave a drink to Mary\"`. You are the person holding the `\" to\"` token, and your goal is to figure out that the person in front of him has the `\" Mary\"` token.\n",
    "\n",
    "To be clear about how this analogy relates to transformers:\n",
    "* Each person in the line represents a vector in the residual stream. Initially they just store their own token, but they accrue more information as they ask questions and receive answers (i.e. as components write to the residual stream)\n",
    "* The operation of an attention head is represented by a question & answer:\n",
    "    * The person who asks is the destination token, the people who answer are the source tokens\n",
    "    * The question is the query vector\n",
    "    * The information *which determines who answers the question* is the key vector\n",
    "    * The information *which gets passed back to the original asker* is the value vector\n",
    "\n",
    "Now, here is how the IOI circuit works in this analogy. Each bullet point represents a class of attention heads.\n",
    "\n",
    "* The person with the second `\" John\"` token asks the question \"does anyone else hold the name `\" John\"`?\". They get a reply from the first `\" John\"` token, who also gives him their location. So he now knows that `\" John\"` is repeated, and he knows that the first `\" John\"` token is 4th in the sequence.\n",
    "    * These are *Duplicate Token Heads*\n",
    "* You ask the question \"which names are repeated?\", and you get an answer from the person holding the second `\" John\"` token. You now also know that `\" John\"` is repeated, and where the first `\" John\"` token is.\n",
    "    * These are *S-Inhibition Heads*\n",
    "* You ask the question \"does anyone have a name that isn't `\" John\"`, and isn't at the 4th position in the sequence?\". You get a reply from the person holding the `\" Mary\"` token, who tells you that they have name `\" Mary\"`. You use this as your prediction.\n",
    "    * These are *Name Mover Heads*\n",
    "\n",
    "This is a fine first-pass understanding of how the circuit works. A few other features:\n",
    "\n",
    "* The person after the first `\" John\"` (holding `\" went\"`) had previously asked about the identity of the person behind him. So he knows that the 4th person in the sequence holds the `\" John\"` token, meaning he can also reply to the question of the person holding the second `\" John\"` token. *(previous token heads / induction heads)*\n",
    "    * This might not seem necessary, but since previous token heads / induction heads are just a pretty useful thing to have in general, it makes sense that you'd want to make use of this information!\n",
    "* If for some reason you forget to ask the question \"does anyone have a name that isn't `\" John\"`, and isn't at the 4th position in the sequence?\", then you'll have another chance to do this.\n",
    "    * These are *(Backup Name Mover Heads)*\n",
    "    * Their existance might be partly because transformers are trained with **dropout**. This can make them \"forget\" things, so it's important to have a backup method for recovering that information!\n",
    "* You want to avoid overconfidence, so you also ask the question \"does anyone have a name that isn't `\" John\"`, and isn't at the 4th position in the sequence?\" another time, in order to ***anti-***predict the response that you get from this question. *(negative name mover heads)*\n",
    "    * Yes, this is as weird as it sounds! The authors speculate that these heads \"hedge\" the predictions, avoiding high cross-entropy loss when making mistakes.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Diagram 1 (simple)</summary>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ioi-main-simple-a.png\" width=\"1000\">\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Diagram 2 (complex)</summary>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ioi-main-full-d.png\" width=\"1250\">\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "to6NugkdtAgi"
   },
   "source": [
    "## Content & Learning Objectives\n",
    "\n",
    "### 1Ô∏è‚É£ Model & Task Setup\n",
    "\n",
    "In this section you'll set up your model, and see how to analyse its performance on the IOI task. You'll also learn how to measure it's performance using tools like logit difference.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand the IOI task, and why the authors chose to study it\n",
    "> * Build functions to demonstrate the model's performance on this task\n",
    "\n",
    "### 2Ô∏è‚É£ Logit Attribution\n",
    "\n",
    "Next, you'll move on to some component attribution: evaluating the importance of each model component for the IOI task. However, this type of analysis is limited to measuring a component's direct effect, as opposed to indirect effect - we'll measure the latter in future sections.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Perform direct logit attribution to figure out which heads are writing to the residual stream in a significant way\n",
    "> * Learn how to use different transformerlens helper functions, which decompose the residual stream in different ways\n",
    "\n",
    "### 3Ô∏è‚É£ Activation Patching\n",
    "\n",
    "We introduce one of the two important patching tools you'll use during this section: **activation patching**. This can be used to discover which components of a model are important for a particular task, by measuring the changes in our previously-defined task metrics when you patch into a particular component with corrupted input.\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand the idea of activation patching, and how it can be used\n",
    ">     * Implement some of the activation patching helper functinos in transformerlens from scratch (i.e. using hooks)\n",
    "> * Use activation patching to track the layers & sequence positions in the residual stream where important information is stored and processed\n",
    "> * By the end of this section, you should be able to draw a rough sketch of the IOI circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7sQUO3PtAgi"
   },
   "source": [
    "## Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XW_E5QQWtAgi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "chapter = \"chapter1_transformer_interp\"\n",
    "repo = \"ARENA_3.0\"\n",
    "branch = \"main\"\n",
    "\n",
    "# Install dependencies\n",
    "try:\n",
    "    import transformer_lens\n",
    "except:\n",
    "    %pip install transformer_lens==2.11.0 einops jaxtyping git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "\n",
    "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
    "root = (\n",
    "    \"/content\"\n",
    "    if IN_COLAB\n",
    "    else \"./\"\n",
    "    if repo not in os.getcwd()\n",
    "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
    ")\n",
    "\n",
    "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
    "    if not IN_COLAB:\n",
    "        #!sudo apt-get install unzip # <- you *may* need to uncomment this, if running locally.\n",
    "        %pip install jupyter ipython --upgrade\n",
    "\n",
    "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
    "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
    "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
    "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
    "        !rm {root}/{branch}.zip\n",
    "        !rmdir {root}/{repo}-{branch}\n",
    "\n",
    "\n",
    "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
    "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
    "\n",
    "os.chdir(f\"{root}/{chapter}/exercises\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7j_7aMV6tAgj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "from typing import Callable, Literal\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch as t\n",
    "from IPython.display import HTML, display\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Column, Table\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer, utils\n",
    "from transformer_lens.components import MLP, Embed, LayerNorm, Unembed\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "device = t.device(\n",
    "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = \"chapter1_transformer_interp\"\n",
    "section = \"part41_indirect_object_identification\"\n",
    "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
    "exercises_dir = root_dir / chapter / \"exercises\"\n",
    "section_dir = exercises_dir / section\n",
    "\n",
    "import part41_indirect_object_identification.tests as tests\n",
    "from plotly_utils import bar, imshow, line, scatter\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbeh1GHetAgj"
   },
   "source": [
    "# 1Ô∏è‚É£ Model & Task Setup\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand the IOI task, and why the authors chose to study it\n",
    "> * Build functions to demonstrate the model's performance on this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpyOUJajtAgj"
   },
   "source": [
    "## Loading our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpGvVKXYtAgj"
   },
   "source": [
    "The first step is to load in our model, GPT-2 Small, a 12 layer and 80M parameter transformer with `HookedTransformer.from_pretrained`. The various flags are simplifications that preserve the model's output but simplify its internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "u3E41FjmtAgj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dPsLl-TtAgj"
   },
   "source": [
    "<details>\n",
    "<summary>Note on <code>refactor_factored_attn_matrices</code> (optional)</summary>\n",
    "\n",
    "This argument means we redefine the matrices $W_Q$, $W_K$, $W_V$ and $W_O$ in the model (without changing the model's actual behaviour).\n",
    "\n",
    "For example, we know that instead of working with $W_Q$ and $W_K$ individually, the only matrix we actually need to use in the model is the low-rank matrix $W_Q W_K^T$ (note that I'm using the convention of matrix multiplication on the right, which matches the code in transformerlens and previous exercises in this series, but doesn't match Anthropic's Mathematical Frameworks paper). So if we perform singular value decomposition $W_Q W_K^T = U S V^T$, then we see that we can just as easily define $W_Q = U \\sqrt{S}$ and $W_K = V \\sqrt{S}$ and use these instead. This means that $W_Q$ and $W_K$ both have orthogonal columns with matching norms. You can investigate this yourself (e.g. using the code below). This is arguably a more interpretable setup, because now there's no obvious asymmetry between the keys and queries.\n",
    "\n",
    "There's also some fiddlyness with how biases are handled in this factorisation, which is why the comments above don't hold absolutely (see the documentation for more info).\n",
    "\n",
    "```python\n",
    "# Show column norms are the same (except first few, for fiddly bias reasons)\n",
    "line([model.W_Q[0, 0].pow(2).sum(0), model.W_K[0, 0].pow(2).sum(0)])\n",
    "# Show columns are orthogonal (except first few, again)\n",
    "W_Q_dot_products = einops.einsum(\n",
    "    model.W_Q[0, 0], model.W_Q[0, 0], \"d_model d_head_1, d_model d_head_2 -> d_head_1 d_head_2\"\n",
    ")\n",
    "imshow(W_Q_dot_products)\n",
    "```\n",
    "\n",
    "In a similar way, since $W_{OV} = W_V W_O = U S V^T$, we can define $W_V = U S$ and $W_O = V^T$. This is arguably a more interpretable setup, because now $W_O$ is just a rotation, and doesn't change the norm, so $z$ has the same norm as the result of the head.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Note on <code>fold_ln</code>, <code>center_unembed</code> and <code>center_writing_weights</code> (optional)</summary>\n",
    "\n",
    "See link [here](https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln) for comments.\n",
    "</details>\n",
    "\n",
    "<div style=\"\n",
    "  border-left: 6px solid #2e8b57;\n",
    "  background: linear-gradient(90deg, #f6fff8 0%, #f0fff4 100%);\n",
    "  padding: 1rem 1.25rem;\n",
    "  margin: 1.5rem 0;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n",
    "\">\n",
    "  <h2 style=\"\n",
    "    color: #2e8b57;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    letter-spacing: 0.5px;\n",
    "  \">AISAA Note</h2>\n",
    "  <p style=\"\n",
    "    margin: 0;\n",
    "    color: #2f3e46;\n",
    "    line-height: 1.5;\n",
    "  \">\n",
    "  The way LayerNorm is handled (and decomposed to aid interpretability) is a rather interesting technical tangent, if you fancied going down a bit of a rabbit hole!\n",
    "      \n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fok9oJortAgj"
   },
   "source": [
    "The next step is to verify that the model can *actually* do the task! Here we use `utils.test_prompt`, and see that the model is significantly better at predicting Mary than John!\n",
    "\n",
    "<details><summary>Asides</summary>\n",
    "\n",
    "Note: If we were being careful, we'd want to run the model on a range of prompts and find the average performance. We'll do more stuff like this in the fourth section (when we try to replicate some of the paper's results, and take a more rigorous approach).\n",
    "\n",
    "`prepend_bos` is a flag to add a BOS (beginning of sequence) to the start of the prompt. GPT-2 was not trained with this, but I find that it often makes model behaviour more stable, as the first token is treated weirdly.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tXXcL5VvtAgj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' John', ' and', ' Mary', ' went', ' to', ' the', ' store', ',', ' John', ' gave', ' a', ' bottle', ' of', ' milk', ' to']\n",
      "Tokenized answer: [' Mary']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.07</span><span style=\"font-weight: bold\">% Token: | Mary|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m70.07\u001b[0m\u001b[1m% Token: | Mary|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 18.09 Prob: 70.07% Token: | Mary|\n",
      "Top 1th token. Logit: 15.38 Prob:  4.67% Token: | the|\n",
      "Top 2th token. Logit: 15.35 Prob:  4.54% Token: | John|\n",
      "Top 3th token. Logit: 15.25 Prob:  4.11% Token: | them|\n",
      "Top 4th token. Logit: 14.84 Prob:  2.73% Token: | his|\n",
      "Top 5th token. Logit: 14.06 Prob:  1.24% Token: | her|\n",
      "Top 6th token. Logit: 13.54 Prob:  0.74% Token: | a|\n",
      "Top 7th token. Logit: 13.52 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 13.13 Prob:  0.49% Token: | Jesus|\n",
      "Top 9th token. Logit: 12.97 Prob:  0.42% Token: | him|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here is where we test on a single prompt\n",
    "# Result: 70% probability on Mary, as we expect\n",
    "\n",
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy0f7_xItAgj"
   },
   "source": [
    "We now want to find a reference prompt to run the model on. Even though our ultimate goal is to reverse engineer how this behaviour is done in general, often the best way to start out in mechanistic interpretability is by zooming in on a concrete example and understanding it in detail, and only *then* zooming out and verifying that our analysis generalises. In section 3, we'll work with a dataset similar to the one used by the paper authors, but this probably wouldn't be the first thing we reached for if we were just doing initial investigations.\n",
    "\n",
    "We'll run the model on 4 instances of this task, each prompt given twice - one with the first name as the indirect object, one with the second name. To make our lives easier, we'll carefully choose prompts with single token names and the corresponding names in the same token positions.\n",
    "\n",
    "<details><summary>Aside on tokenization</summary>\n",
    "\n",
    "We want models that can take in arbitrary text, but models need to have a fixed vocabulary. So the solution is to define a vocabulary of **tokens** and to deterministically break up arbitrary text into tokens. Tokens are, essentially, subwords, and are determined by finding the most frequent substrings - this means that tokens vary a lot in length and frequency!\n",
    "\n",
    "Tokens are a *massive* headache and are one of the most annoying things about reverse engineering language models... Different names will be different numbers of tokens, different prompts will have the relevant tokens at different positions, different prompts will have different total numbers of tokens, etc. Language models often devote significant amounts of parameters in early layers to convert inputs from tokens to a more sensible internal format (and do the reverse in later layers). You really, really want to avoid needing to think about tokenization wherever possible when doing exploratory analysis (though, of course, it's relevant later when trying to flesh out your analysis and make it rigorous!). HookedTransformer comes with several helper methods to deal with tokens: `to_tokens, to_string, to_str_tokens, to_single_token, get_token_position`\n",
    "\n",
    "**Exercise:** I recommend using `model.to_str_tokens` to explore how the model tokenizes different strings. In particular, try adding or removing spaces at the start, or changing capitalization - these change tokenization!</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iFsYo9OutAgj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'When John and Mary went to the shops, John gave the bag to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'When John and Mary went to the shops, Mary gave the bag to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'When Tom and James went to the park, James gave the ball to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'When Tom and James went to the park, Tom gave the ball to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'When Dan and Sid went to the shops, Sid gave an apple to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'When Dan and Sid went to the shops, Dan gave an apple to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'After Martin and Amy went to the park, Amy gave a drink to'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'After Martin and Amy went to the park, Martin gave a drink to'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'When John and Mary went to the shops, John gave the bag to'\u001b[0m,\n",
       "    \u001b[32m'When John and Mary went to the shops, Mary gave the bag to'\u001b[0m,\n",
       "    \u001b[32m'When Tom and James went to the park, James gave the ball to'\u001b[0m,\n",
       "    \u001b[32m'When Tom and James went to the park, Tom gave the ball to'\u001b[0m,\n",
       "    \u001b[32m'When Dan and Sid went to the shops, Sid gave an apple to'\u001b[0m,\n",
       "    \u001b[32m'When Dan and Sid went to the shops, Dan gave an apple to'\u001b[0m,\n",
       "    \u001b[32m'After Martin and Amy went to the park, Amy gave a drink to'\u001b[0m,\n",
       "    \u001b[32m'After Martin and Amy went to the park, Martin gave a drink to'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' John'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' John'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' Mary'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Tom'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' James'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' James'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' Tom'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Dan'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' Sid'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Sid'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' Dan'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Martin'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' Amy'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Amy'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">' Martin'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' Mary'\u001b[0m, \u001b[32m' John'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' John'\u001b[0m, \u001b[32m' Mary'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' Tom'\u001b[0m, \u001b[32m' James'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' James'\u001b[0m, \u001b[32m' Tom'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' Dan'\u001b[0m, \u001b[32m' Sid'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' Sid'\u001b[0m, \u001b[32m' Dan'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' Martin'\u001b[0m, \u001b[32m' Amy'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\u001b[32m' Amy'\u001b[0m, \u001b[32m' Martin'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5335</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1757</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1757</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5335</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4186</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3700</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3700</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4186</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6035</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15686</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15686</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6035</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5780</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14235</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14235</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5780</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cuda:0'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m5335\u001b[0m,  \u001b[1;36m1757\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m1757\u001b[0m,  \u001b[1;36m5335\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m4186\u001b[0m,  \u001b[1;36m3700\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m3700\u001b[0m,  \u001b[1;36m4186\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m6035\u001b[0m, \u001b[1;36m15686\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m15686\u001b[0m,  \u001b[1;36m6035\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m \u001b[1;36m5780\u001b[0m, \u001b[1;36m14235\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[1m[\u001b[0m\u001b[1;36m14235\u001b[0m,  \u001b[1;36m5780\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                   Prompts &amp; Answers:                                    </span>\n",
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Prompt                                                        </span>‚îÉ<span style=\"font-weight: bold\"> Correct   </span>‚îÉ<span style=\"font-weight: bold\"> Incorrect </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ When John and Mary went to the shops, John gave the bag to    ‚îÇ ' Mary'   ‚îÇ ' John'   ‚îÇ\n",
       "‚îÇ When John and Mary went to the shops, Mary gave the bag to    ‚îÇ ' John'   ‚îÇ ' Mary'   ‚îÇ\n",
       "‚îÇ When Tom and James went to the park, James gave the ball to   ‚îÇ ' Tom'    ‚îÇ ' James'  ‚îÇ\n",
       "‚îÇ When Tom and James went to the park, Tom gave the ball to     ‚îÇ ' James'  ‚îÇ ' Tom'    ‚îÇ\n",
       "‚îÇ When Dan and Sid went to the shops, Sid gave an apple to      ‚îÇ ' Dan'    ‚îÇ ' Sid'    ‚îÇ\n",
       "‚îÇ When Dan and Sid went to the shops, Dan gave an apple to      ‚îÇ ' Sid'    ‚îÇ ' Dan'    ‚îÇ\n",
       "‚îÇ After Martin and Amy went to the park, Amy gave a drink to    ‚îÇ ' Martin' ‚îÇ ' Amy'    ‚îÇ\n",
       "‚îÇ After Martin and Amy went to the park, Martin gave a drink to ‚îÇ ' Amy'    ‚îÇ ' Martin' ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                   Prompts & Answers:                                    \u001b[0m\n",
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mPrompt                                                       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mCorrect  \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mIncorrect\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ When John and Mary went to the shops, John gave the bag to    ‚îÇ ' Mary'   ‚îÇ ' John'   ‚îÇ\n",
       "‚îÇ When John and Mary went to the shops, Mary gave the bag to    ‚îÇ ' John'   ‚îÇ ' Mary'   ‚îÇ\n",
       "‚îÇ When Tom and James went to the park, James gave the ball to   ‚îÇ ' Tom'    ‚îÇ ' James'  ‚îÇ\n",
       "‚îÇ When Tom and James went to the park, Tom gave the ball to     ‚îÇ ' James'  ‚îÇ ' Tom'    ‚îÇ\n",
       "‚îÇ When Dan and Sid went to the shops, Sid gave an apple to      ‚îÇ ' Dan'    ‚îÇ ' Sid'    ‚îÇ\n",
       "‚îÇ When Dan and Sid went to the shops, Dan gave an apple to      ‚îÇ ' Sid'    ‚îÇ ' Dan'    ‚îÇ\n",
       "‚îÇ After Martin and Amy went to the park, Amy gave a drink to    ‚îÇ ' Martin' ‚îÇ ' Amy'    ‚îÇ\n",
       "‚îÇ After Martin and Amy went to the park, Martin gave a drink to ‚îÇ ' Amy'    ‚îÇ ' Martin' ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_format = [\n",
    "    \"When John and Mary went to the shops,{} gave the bag to\",\n",
    "    \"When Tom and James went to the park,{} gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops,{} gave an apple to\",\n",
    "    \"After Martin and Amy went to the park,{} gave a drink to\",\n",
    "]\n",
    "name_pairs = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "\n",
    "# Define 8 prompts, in 4 groups of 2 (with adjacent prompts having answers swapped)\n",
    "prompts = [\n",
    "    prompt.format(name)\n",
    "    for (prompt, names) in zip(prompt_format, name_pairs)\n",
    "    for name in names[::-1]\n",
    "]\n",
    "# Define the answers for each prompt, in the form (correct, incorrect)\n",
    "answers = [names[::i] for names in name_pairs for i in (1, -1)]\n",
    "# Define the answer tokens (same shape as the answers)\n",
    "answer_tokens = t.concat([model.to_tokens(names, prepend_bos=False).T for names in answers])\n",
    "\n",
    "rprint(prompts)\n",
    "rprint(answers)\n",
    "rprint(answer_tokens)\n",
    "\n",
    "table = Table(\"Prompt\", \"Correct\", \"Incorrect\", title=\"Prompts & Answers:\")\n",
    "\n",
    "for prompt, answer in zip(prompts, answers):\n",
    "    table.add_row(prompt, repr(answer[0]), repr(answer[1]))\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hvi1d5ntAgj"
   },
   "source": [
    "<details>\n",
    "<summary>Aside - the <code>rich</code> library</summary>\n",
    "\n",
    "The outputs above were created by `rich`, a fun library which prints things in nice formats. It has functions like `rich.table.Table`, which are very easy to use but can produce visually clear outputs which are sometimes useful.\n",
    "\n",
    "You can also color the columns of a table, by using the `rich.table.Column` argument with the `style` parameter:\n",
    "\n",
    "```python\n",
    "cols = [\n",
    "    \"Prompt\",\n",
    "    Column(\"Correct\", style=\"rgb(0,200,0) bold\"),\n",
    "    Column(\"Incorrect\", style=\"rgb(255,0,0) bold\"),\n",
    "]\n",
    "table = Table(*cols, title=\"Prompts & Answers:\")\n",
    "\n",
    "for prompt, answer in zip(prompts, answers):\n",
    "    table.add_row(prompt, repr(answer[0]), repr(answer[1]))\n",
    "\n",
    "rprint(table)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMALKvpftAgj"
   },
   "source": [
    "We now run the model on these prompts and use `run_with_cache` to get both the logits and a cache of all internal activations for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AxUCWLKOtAgj"
   },
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "# Move the tokens to the GPU\n",
    "tokens = tokens.to(device)\n",
    "# Run the model and cache all activations\n",
    "original_logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'When', ' Tom', ' and', ' James', ' went', ' to', ' the', ' park', ',', ' James', ' gave', ' the', ' ball', ' to']\n",
      "Tokenized answer: [' Tom']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.88</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40.01</span><span style=\"font-weight: bold\">% Token: | Tom|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.88\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m40.01\u001b[0m\u001b[1m% Token: | Tom|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.88 Prob: 40.01% Token: | Tom|\n",
      "Top 1th token. Logit: 14.44 Prob:  9.44% Token: | the|\n",
      "Top 2th token. Logit: 14.12 Prob:  6.88% Token: | his|\n",
      "Top 3th token. Logit: 13.34 Prob:  3.15% Token: | them|\n",
      "Top 4th token. Logit: 13.26 Prob:  2.90% Token: | a|\n",
      "Top 5th token. Logit: 13.17 Prob:  2.66% Token: | James|\n",
      "Top 6th token. Logit: 12.76 Prob:  1.76% Token: | their|\n",
      "Top 7th token. Logit: 12.39 Prob:  1.22% Token: | him|\n",
      "Top 8th token. Logit: 11.64 Prob:  0.58% Token: | John|\n",
      "Top 9th token. Logit: 11.55 Prob:  0.52% Token: | George|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Tom'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Tom'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt('When Tom and James went to the park, James gave the ball to', ' Tom', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngb4prqOtAgj"
   },
   "source": [
    "We'll later be evaluating how model performance differs upon performing various interventions, so it's useful to have a metric to measure model performance. Our metric here will be the **logit difference**, the difference in logit between the indirect object's name and the subject's name (eg, `logit(Mary) - logit(John)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRUjI6PItAgj"
   },
   "source": [
    "### Exercise - implement the performance evaluation function\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥üî¥‚ö™‚ö™\n",
    "> Importance: üîµüîµüîµüîµ‚ö™\n",
    ">\n",
    "> It's important to understand exactly what this function is computing, and why it matters.\n",
    "> ```\n",
    "\n",
    "This function should take in your model's logit output (shape `(batch, seq, d_vocab)`), and the array of answer tokens (shape `(batch, 2)`, containing the token ids of correct and incorrect answers respectively for each sequence), and return the logit difference as described above. If `per_prompt` is False, then it should take the mean over the batch dimension, if not then it should return an array of length `batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA2n69TNtAgj"
   },
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Int[Tensor, \"batch 2\"] = answer_tokens,\n",
    "    per_prompt: bool = False,\n",
    ") -> Float[Tensor, \"*batch\"]:\n",
    "    \"\"\"\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "tests.test_logits_to_ave_logit_diff(logits_to_ave_logit_diff)\n",
    "\n",
    "original_per_prompt_diff = logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True)\n",
    "print(\"Per prompt logit difference:\", original_per_prompt_diff)\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n",
    "print(\"Average logit difference:\", original_average_logit_diff)\n",
    "\n",
    "cols = [\n",
    "    \"Prompt\",\n",
    "    Column(\"Correct\", style=\"rgb(0,200,0) bold\"),\n",
    "    Column(\"Incorrect\", style=\"rgb(255,0,0) bold\"),\n",
    "    Column(\"Logit Difference\", style=\"bold\"),\n",
    "]\n",
    "table = Table(*cols, title=\"Logit differences\")\n",
    "\n",
    "for prompt, answer, logit_diff in zip(prompts, answers, original_per_prompt_diff):\n",
    "    table.add_row(prompt, repr(answer[0]), repr(answer[1]), f\"{logit_diff.item():.3f}\")\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgUlwMZJtAgj"
   },
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def logits_to_ave_logit_diff(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Int[Tensor, \"batch 2\"] = answer_tokens,\n",
    "    per_prompt: bool = False,\n",
    ") -> Float[Tensor, \"*batch\"]:\n",
    "    \"\"\"\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    \"\"\"\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits: Float[Tensor, \"batch d_vocab\"] = logits[:, -1, :]\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    answer_logits: Float[Tensor, \"batch 2\"] = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    # Find logit difference\n",
    "    correct_logits, incorrect_logits = answer_logits.unbind(dim=-1)\n",
    "    answer_logit_diff = correct_logits - incorrect_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wYOcwYZtAgj"
   },
   "source": [
    "# 2Ô∏è‚É£ Logit Attribution\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Perform direct logit attribution to figure out which heads are writing to the residual stream in a significant way\n",
    "> * Learn how to use different transformerlens helper functions, which decompose the residual stream in different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSfeRE61tAgk"
   },
   "source": [
    "## Direct Logit Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJjgSLiatAgk"
   },
   "source": [
    "The easiest part of the model to understand is the output - this is what the model is trained to optimize, and so it can always be directly interpreted! Often the right approach to reverse engineering a circuit is to start at the end, understand how the model produces the right answer, and to then work backwards (you will have seen this if you went through the balanced bracket classifier task, and in fact if you did then this section will probably be quite familiar to you and you should feel free to just skim through it). The main technique used to do this is called **direct logit attribution**\n",
    "\n",
    "**Background:** The central object of a transformer is the **residual stream**. This is the sum of the outputs of each layer and of the original token and positional embedding. Importantly, this means that any linear function of the residual stream can be perfectly decomposed into the contribution of each layer of the transformer. Further, each attention layer's output can be broken down into the sum of the output of each head (See [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) for details), and each MLP layer's output can be broken down into the sum of the output of each neuron (and a bias term for each layer).\n",
    "\n",
    "The logits of a model are `logits=Unembed(LayerNorm(final_residual_stream))`. The Unembed is a linear map, and LayerNorm is approximately a linear map, so we can decompose the logits into the sum of the contributions of each component, and look at which components contribute the most to the logit of the correct token! This is called **direct logit attribution**. Here we look at the direct attribution to the logit difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_hflRtAgk"
   },
   "source": [
    "### Background and motivation of the logit difference\n",
    "\n",
    "Logit difference is actually a *really* nice and elegant metric and is a particularly nice aspect of the setup of Indirect Object Identification. In general, there are two natural ways to interpret the model's outputs: the output logits, or the output log probabilities (or probabilities).\n",
    "\n",
    "The logits are much nicer and easier to understand, as noted above. However, the model is trained to optimize the cross-entropy loss (the average of log probability of the correct token). This means it does not directly optimize the logits, and indeed if the model adds an arbitrary constant to every logit, the log probabilities are unchanged.\n",
    "\n",
    "But we have:\n",
    "\n",
    "```\n",
    "log_probs == logits.log_softmax(dim=-1) == logits - logsumexp(logits)\n",
    "```\n",
    "\n",
    "and because they differ by a constant, we have:\n",
    "\n",
    "```\n",
    "log_probs(\" Mary\") - log_probs(\" John\") = logits(\" Mary\") - logits(\" John\")\n",
    "```\n",
    "\n",
    "- the ability to add an arbitrary constant cancels out!\n",
    "\n",
    "<details>\n",
    "<summary>Technical details (if this equivalence doesn't seem obvious to you)</summary>\n",
    "\n",
    "Let $\\vec{\\textbf{x}}$ be the logits, $\\vec{\\textbf{L}}$ be the log probs, and $\\vec{\\textbf{p}}$ be the probs. Then we have the following relations:\n",
    "\n",
    "$$\n",
    "p_i = \\operatorname{softmax}(\\vec{\\textbf{x}})_i = \\frac{e^{x_i}}{\\sum_{i=1}^n e^{x_i}}\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "L_i = \\log p_i\n",
    "$$\n",
    "\n",
    "Combining these, we get:\n",
    "\n",
    "$$\n",
    "L_i = \\log \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}} = x_i - \\log \\sum_{j=1}^n e^{x_j}\n",
    "$$\n",
    "\n",
    "Notice that the sum term on the right hand side is the same for all $i$, so we get:\n",
    "\n",
    "$$\n",
    "L_i - L_j = x_i - x_j\n",
    "$$\n",
    "\n",
    "in other words, the logit diff $x_i - x_j$ is the same as the log prob diff. This motivates the choice of logit diff as our choice of metric (since the model is directly training to make the log prob of the correct token large, and all other log probs small).\n",
    "\n",
    "</details>\n",
    "\n",
    "Further, the metric helps us isolate the precise capability we care about - figuring out *which* name is the Indirect Object. There are many other components of the task - deciding whether to return an article (the) or pronoun (her) or name, realising that the sentence wants a person next at all, etc. By taking the logit difference we control for all of that.\n",
    "\n",
    "Our metric is further refined, because each prompt is repeated twice, for each possible indirect object. This controls for irrelevant behaviour such as the model learning that John is a more frequent token than Mary (this actually happens! The final layernorm bias increases the John logit by 1 relative to the Mary logit). Another way to handle this would be to use a large enough dataset (with names randomly chosen) that this effect is averaged out, which is what we'll do in section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pB-CHoaetAgk"
   },
   "source": [
    "<details> <summary>Ignoring LayerNorm</summary>\n",
    "\n",
    "LayerNorm is an analogous normalization technique to BatchNorm (that's friendlier to massive parallelization) that transformers use. Every time a transformer layer reads information from the residual stream, it applies a LayerNorm to normalize the vector at each position (translating to set the mean to 0 and scaling to set the variance to 1) and then applying a learned vector of weights and biases to scale and translate the normalized vector. This is *almost* a linear map, apart from the scaling step, because that divides by the norm of the vector and the norm is not a linear function. (The `fold_ln` flag when loading a model factors out all the linear parts).\n",
    "\n",
    "But if we fixed the scale factor, the LayerNorm would be fully linear. And the scale of the residual stream is a global property that's a function of *all* components of the stream, while in practice there is normally just a few directions relevant to any particular component, so in practice this is an acceptable approximation. So when doing direct logit attribution we use the `apply_ln` flag on the `cache` to apply the global layernorm scaling factor to each constant. See [my clean GPT-2 implementation](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=Clean_Transformer_Implementation) for more on LayerNorm.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2GRWK3ptAgk"
   },
   "source": [
    "### Logit diff directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ul8Uz7ltAgk"
   },
   "source": [
    "***Getting an output logit is equivalent to projecting onto a direction in the residual stream, and the same is true for getting the logit diff.***\n",
    "\n",
    "<details>\n",
    "<summary>If it's not clear what is meant by this statement, read this dropdown.</summary>\n",
    "\n",
    "Suppose our final value in the residual stream for a single sequence and a position within that sequence is $x$ (i.e. $x$ is a vector of length $d_{model}$). Then (ignoring layernorm - see the point above for why it's okay to do this), we get logits by multiplying by the unembedding matrix $W_U$ (which has shape $(d_{model}, d_{vocab})$):\n",
    "\n",
    "$$\n",
    "\\text{output} = x^T W_U\n",
    "$$\n",
    "\n",
    "Now, remember that we want the logit diff, which is $\\text{output}_{IO} - \\text{output}_{S}$ (the difference between the logits for our indirect object and subject). We can write this as:\n",
    "\n",
    "$$\n",
    "\\text{logit diff} = (x^T W_U)_{IO} - (x^T W_U)_{S} = x^T (u_{IO} - u_{S})\n",
    "$$\n",
    "\n",
    "where $u_{IO}$ and $u_S$ are the **columns of the unembedding matrix** $W_U$ corresponding to the indirect object and subject tokens respectively.\n",
    "\n",
    "To summarize, we've written the logit diff as a dot product between the vector in the residual stream and a constant vector (which is a function of the model's unembedding matrix). We call this vector $u_{IO} - u_{S}$ the **logit difference direction** (because it *\"points in the direction of largest logit difference\"*). To put it another way, if $x$ is a vector of fixed magnitude, then it maximises the logit difference when it is pointing in the same direction as the vector $u_{IO} - u_{S}$. We use the term \"projection\" synonymously with \"dot product\" here.\n",
    "\n",
    "(If you've completed the exercise where we interpret a transformer on balanced / unbalanced bracket strings, this is basically the same principle. The only difference here is that we actually have a much larger unembedding vocabulary than just the classifications `{balanced, unbalanced}`, but since we're only interested in comparing the model's prediction for IO vs S, and the logits for these two tokens are usually larger than most others, this method is still well-justified).\n",
    "</details>\n",
    "\n",
    "We use `model.tokens_to_residual_directions` to map the answer tokens to that direction, and then convert this to a logit difference direction for each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  border-left: 6px solid #2e8b57;\n",
    "  background: linear-gradient(90deg, #f6fff8 0%, #f0fff4 100%);\n",
    "  padding: 1rem 1.25rem;\n",
    "  margin: 1.5rem 0;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n",
    "\">\n",
    "  <h2 style=\"\n",
    "    color: #2e8b57;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    letter-spacing: 0.5px;\n",
    "  \">AISAA Note</h2>\n",
    "  <p style=\"margin: 0; color: #2f3e46; line-height: 1.5;\">\n",
    "  This is somewhat reminiscent of the benefits of old-school interpretability methods in the form of simple linear models--the output being simply a linear combination of \"independent\" terms whose contribution to the logits we can directly read off. Note that there is a sense in which what is happening in Transformers is a kind of a classification task: for a particular input token, we're mapping to a probability distribution over a fixed set of next tokens. So I think this analogy offers some value!\n",
    "  </p>\n",
    "</br>\n",
    "  <p style=\"margin: 0; color: #2f3e46; line-height: 1.5;\">\n",
    "  However, an important caveat is that we're only measuring the *direct* influence here; a components' contribution might have indirect effects via later layers that we won't be able to capture with this technique.\n",
    "      \n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH-9p5LztAgk"
   },
   "outputs": [],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)  # [batch 2 d_model]\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "correct_residual_directions, incorrect_residual_directions = answer_residual_directions.unbind(\n",
    "    dim=1\n",
    ")\n",
    "logit_diff_directions = (\n",
    "    correct_residual_directions - incorrect_residual_directions\n",
    ")  # [batch d_model]\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXoNJbqYtAgk"
   },
   "source": [
    "To verify that this works, we can apply this to the final residual stream for our cached prompts (after applying LayerNorm scaling) and verify that we get the same answer.\n",
    "\n",
    "<details> <summary>Technical details</summary>\n",
    "\n",
    "`logits = Unembed(LayerNorm(final_residual_stream))`, so we technically need to account for the centering, and then learned translation and scaling of the layernorm, not just the variance 1 scaling.\n",
    "\n",
    "The centering is accounted for with the preprocessing flag `center_writing_weights` which ensures that every weight matrix writing to the residual stream has mean zero.\n",
    "\n",
    "The learned scaling is folded into the unembedding weights `model.unembed.W_U` via `W_U_fold = layer_norm.weights[:, None] * unembed.W_U`\n",
    "\n",
    "The learned translation is folded to `model.unembed.b_U`, a bias added to the logits (note that GPT-2 is not trained with an existing `b_U`). This roughly represents unigram statistics. But we can ignore this because each prompt occurs twice with names in the opposite order, so this perfectly cancels out.\n",
    "\n",
    "Note that rather than using layernorm scaling we could just study cache[\"ln_final.hook_normalised\"]\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V94iLVcmtAgk"
   },
   "source": [
    "The code below does the following:\n",
    "\n",
    "* Gets the final residual stream values from the `cache` object (which you should already have defined above).\n",
    "* Apply layernorm scaling to these values.\n",
    "    * This is done by `cache.apply_to_ln_stack`, a helpful function which takes a stack of residual stream values (e.g. a batch, or the residual stream decomposed into components), treats them as the input to a specific layer, and applies the layer norm scaling of that layer to them.\n",
    "    * The keyword arguments here indicate that our input is the residual stream values for the last sequence position, and we want to apply the final layernorm in the model.\n",
    "* Project them along the unembedding directions (you've already defined these above, as `logit_diff_directions`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9rVNIT8tAgm"
   },
   "outputs": [],
   "source": [
    "# Cache syntax: resid_post is the residual stream at the end of the layer, -1 gets the final layer.\n",
    "# The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream: Float[Tensor, \"batch seq d_model\"] = cache[\"resid_post\", -1]\n",
    "print(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[:, -1, :]\n",
    "\n",
    "# Apply LayerNorm scaling (to just the final sequence position)\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einops.einsum(\n",
    "    scaled_final_token_residual_stream, logit_diff_directions, \"batch d_model, batch d_model ->\"\n",
    ") / len(prompts)\n",
    "\n",
    "print(f\"Calculated average logit diff: {average_logit_diff:.10f}\")\n",
    "print(f\"Original logit difference:     {original_average_logit_diff:.10f}\")\n",
    "\n",
    "t.testing.assert_close(average_logit_diff, original_average_logit_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVN7tLbctAgm"
   },
   "source": [
    "## Logit Lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8073zxttAgm"
   },
   "source": [
    "We can now decompose the residual stream! First we apply a technique called the [**logit lens**](https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) - this looks at the residual stream after each layer and calculates the logit difference from that. This simulates what happens if we delete all subsequence layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcc7fQBYtAgm"
   },
   "source": [
    "### Exercise - implement `residual_stack_to_logit_diff`\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥üî¥‚ö™‚ö™\n",
    "> Importance: üîµüîµüîµ‚ö™‚ö™\n",
    ">\n",
    "> Again, make sure you understand what the output of this function represents.\n",
    "> ```\n",
    "\n",
    "This function should look a lot like your code immediately above. `residual_stack` is a tensor of shape `(..., batch, d_model)` containing the residual stream values for the final sequence position. You should apply the final layernorm to these values, then project them in the logit difference directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    "    logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    ") -> Float[Tensor, \"...\"]:\n",
    "    \"\"\"\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given stack of\n",
    "    components in the residual stream.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# Test function by checking that it gives the same result as the original logit difference\n",
    "t.testing.assert_close(\n",
    "    residual_stack_to_logit_diff(final_token_residual_stream, cache), original_average_logit_diff\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EV8_ALHtAgn"
   },
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    "    logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    ") -> Float[Tensor, \"...\"]:\n",
    "    \"\"\"\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given stack of\n",
    "    components in the residual stream.\n",
    "    \"\"\"\n",
    "    batch_size = residual_stack.size(-2)\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "    return (\n",
    "        einops.einsum(\n",
    "            scaled_residual_stack, logit_diff_directions, \"... batch d_model, batch d_model -> ...\"\n",
    "        )\n",
    "        / batch_size\n",
    "    )\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y7mTC3ZtAgn"
   },
   "source": [
    "Once you have the solution, you can plot your results.\n",
    "\n",
    "<details> <summary>Details on <code>accumulated_resid</code></summary>\n",
    "\n",
    "Key for the plot below: `n_pre` means the residual stream at the start of layer n, `n_mid` means the residual stream after the attention part of layer n (`n_post` is the same as `n+1_pre` so is not included)\n",
    "\n",
    "* `layer` is the layer for which we input the residual stream (this is used to identify *which* layer norm scaling factor we want)\n",
    "* `incl_mid` is whether to include the residual stream in the middle of a layer, ie after attention & before MLP\n",
    "* `pos_slice` is the subset of the positions used. See `utils.Slice` for details on the syntax.\n",
    "* `return_labels` is whether to return the labels for each component returned (useful for plotting)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yt1J_TshtAgn"
   },
   "outputs": [],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "# accumulated_residual has shape (component, batch, d_model)\n",
    "\n",
    "logit_lens_logit_diffs: Float[Tensor, \"component\"] = residual_stack_to_logit_diff(\n",
    "    accumulated_residual, cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "line(\n",
    "    logit_lens_logit_diffs.detach().cpu().numpy(),\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Accumulated Residual Stream\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6vVgFTRtAgn"
   },
   "source": [
    "<details>\n",
    "<summary>Question - what is the interpretation of this plot? What does this tell you about how the model solves this task?</summary>\n",
    "\n",
    "Fascinatingly, we see that the model is utterly unable to do the task until layer 7, almost all performance comes from attention layer 9, and performance actually *decreases* from there.\n",
    "\n",
    "This tells us that there must be something going on (primarily in layers 7, 8 and 9) which writes to the residual stream in the correct way to solve the IOI task. This allows us to narrow in our focus, and start asking questions about what kind of computation is going on in those layers (e.g. the contribution of attention layers vs MLPs, and which attention heads are most important).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzjwWaG3tAgn"
   },
   "source": [
    "## Layer Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qgnL9UytAgn"
   },
   "source": [
    "We can repeat the above analysis but for each layer (this is equivalent to the differences between adjacent residual streams)\n",
    "\n",
    "Note: Annoying terminology overload - layer k of a transformer means the kth **transformer block**, but each block consists of an **attention layer** (to move information around) *and* an **MLP layer** (to process information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g6teeLwtAgn"
   },
   "outputs": [],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "\n",
    "line(\n",
    "    per_layer_logit_diffs,\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Each Layer\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEZ_HYyItAgn"
   },
   "source": [
    "<details>\n",
    "<summary>Question - what is the interpretation of this plot? What does this tell you about how the model solves this task?</summary>\n",
    "\n",
    "We see that only attention layers matter, which makes sense! The IOI task is about moving information around (i.e. moving the correct name and not the incorrect name), and less about processing it. And again we note that attention layer 9 improves things a lot, while attention 10 and attention 11 *decrease* performance.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk-VRRp8tAgn"
   },
   "source": [
    "## Head Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Am55ZovtAgn"
   },
   "source": [
    "We can further break down the output of each attention layer into the sum of the outputs of each attention head. Each attention layer consists of 12 heads, which each act independently and additively.\n",
    "\n",
    "<details> <summary>Decomposing attention output into sums of heads</summary>\n",
    "\n",
    "The standard way to compute the output of an attention layer is by concatenating the mixed values of each head, and multiplying by a big output weight matrix. But as described in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) this is equivalent to splitting the output weight matrix into a per-head output (here `model.blocks[k].attn.W_O`) and adding them up (including an overall bias term for the entire layer).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pb__yj-tAgn"
   },
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_head_residual = einops.rearrange(\n",
    "    per_head_residual, \"(layer head) ... -> layer head ...\", layer=model.cfg.n_layers\n",
    ")\n",
    "print(per_head_residual.shape) # [layer, attn_head, batch, resid_dimension]\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "\n",
    "imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    width=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueuuERHStAgn"
   },
   "source": [
    "We see that only a few heads really matter - heads 9.6 and 9.9 contribute a lot positively (explaining why attention layer 9 is so important), while heads 10.7 and 11.10 contribute a lot negatively (explaining why attention layer 10 and layer 11 are actively harmful). These correspond to (some of) the name movers and negative name movers discussed in the paper. There are also several heads that matter positively or negatively but less strongly (other name movers and backup name movers).\n",
    "\n",
    "There are a few meta observations worth making here - our model has 144 heads, yet we could localise this behaviour to a handful of specific heads, using straightforward, general techniques. This supports the claim in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) that attention heads are the right level of abstraction to understand attention. It also really surprising that there are *negative* heads - eg 10.7 makes the incorrect logit 7x *more* likely. I'm not sure what's going on there, though the paper discusses some possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SGJSQD9tAgn"
   },
   "source": [
    "## Recap of useful functions from this section\n",
    "\n",
    "Here, we take stock of all the functions from transformerlens which you might not have seen previously.\n",
    "\n",
    "* `cache.apply_ln_to_stack`\n",
    "    * Apply layernorm scaling to a stack of residual stream values.\n",
    "    * We used this to help us go from \"final value in residual stream\" to \"projection of logits in logit difference directions\", without getting the code too messy!\n",
    "* `cache.accumulated_resid(layer=None)`\n",
    "    * Returns the accumulated residual stream up to layer `layer` (or up to the final value of residual stream if layer is None), i.e. a stack of previous residual streams up to that layer's input.\n",
    "    * Useful when studying the **logit lens**.\n",
    "    * First dimension of output is `(0_pre, 0_mid, 1_pre, 1_mid, ..., final_post)`\n",
    "* `cache.decompose_resid(layer)`.\n",
    "    * Decomposes the residual stream input to layer `layer` into a stack of the output of previous layers. The sum of these is the input to layer `layer`.\n",
    "    * First dimension of output is `(embed, pos_embed, 0_attn_out, 0_mlp_out, ...)`.\n",
    "* `cache.stack_head_results(layer)`\n",
    "    * Returns a stack of all head results (i.e. residual stream contribution) up to layer `layer`\n",
    "    * (i.e. like `decompose_resid` except it splits each attention layer by head rather than splitting each layer by attention/MLP)\n",
    "    * First dimension of output is `layer * head` (we needed to rearrange to `(layer, head)` to plot it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajbOYAcjtAgn"
   },
   "source": [
    "## Attention Analysis\n",
    "\n",
    "Attention heads are particularly fruitful to study because we can look directly at their attention patterns and study from what positions they move information from and to. This is particularly useful here as we're looking at the direct effect on the logits so we need only look at the attention patterns from the final token.\n",
    "\n",
    "We use the `circuitsvis` library (developed from Anthropic's PySvelte library) to visualize the attention patterns! We visualize the top 3 positive and negative heads by direct logit attribution, and show these for the first prompt (as an illustration).\n",
    "\n",
    "<details> <summary>Interpreting Attention Patterns</summary>\n",
    "\n",
    "A common mistake to make when looking at attention patterns is thinking that they must convey information about the *token* looked at (maybe accounting for the context of the token). But actually, all we can confidently say is that it moves information from the *residual stream position* corresponding to that input token. Especially later on in the model, there may be components in the residual stream that are nothing to do with the input token! Eg the period at the end of a sentence may contain summary information for that sentence, and the head may solely move that, rather than caring about whether it ends in \".\", \"!\" or \"?\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6FD3qVytAgn"
   },
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "def topk_of_Nd_tensor(tensor: Float[Tensor, \"rows cols\"], k: int):\n",
    "    \"\"\"\n",
    "    Helper function: does same as tensor.topk(k).indices, but works over 2D tensors.\n",
    "    Returns a list of indices, i.e. shape [k, tensor.ndim].\n",
    "\n",
    "    Example: if tensor is 2D array of values for each head in each layer, this will\n",
    "    return a list of heads.\n",
    "    \"\"\"\n",
    "    i = t.topk(tensor.flatten(), k).indices\n",
    "    return np.array(np.unravel_index(utils.to_numpy(i), tensor.shape)).T.tolist()\n",
    "\n",
    "\n",
    "k = 3\n",
    "\n",
    "for head_type in [\"Positive\", \"Negative\"]:\n",
    "    # Get the heads with largest (or smallest) contribution to the logit difference\n",
    "    top_heads = topk_of_Nd_tensor(\n",
    "        per_head_logit_diffs * (1 if head_type == \"Positive\" else -1), k\n",
    "    )\n",
    "\n",
    "    # Get all their attention patterns\n",
    "    attn_patterns_for_important_heads: Float[Tensor, \"head q k\"] = t.stack(\n",
    "        [cache[\"pattern\", layer][:, head][0] for layer, head in top_heads]\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    display(HTML(f\"<h2>Top {k} {head_type} Logit Attribution Heads</h2>\"))\n",
    "    display(\n",
    "        cv.attention.attention_patterns(\n",
    "            attention=attn_patterns_for_important_heads,\n",
    "            tokens=model.to_str_tokens(tokens[0]),\n",
    "            #attention_head_names=[f\"{layer}.{head}\" for layer, head in top_heads],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbStOkxNtAgn"
   },
   "source": [
    "Reminder - you can use `attention_patterns` or `attention_heads` for these visuals. The former lets you see the actual values, the latter lets you hover over tokens in a printed sentence (and it provides other useful features like locking on tokens, or a superposition of all heads in the display). Both can be useful in different contexts (although I'd recommend usually using `attention_patterns`, it's more useful in most cases for quickly getting a sense of attention patterns).\n",
    "\n",
    "Try replacing `attention_patterns` above with `attention_heads`, and compare the output.\n",
    "\n",
    "<details>\n",
    "<summary>Help - my <code>attention_heads</code> plots are behaving weirdly.</summary>\n",
    "\n",
    "This seems to be a bug in `circuitsvis` - on VSCode, the attention head plots continually shrink in size.\n",
    "\n",
    "Until this is fixed, one way to get around it is to open the plots in your browser. You can do this inline with the `webbrowser` library:\n",
    "\n",
    "```python\n",
    "attn_heads = cv.attention.attention_heads(\n",
    "    attention = attn_patterns_for_important_heads,\n",
    "    tokens = model.to_str_tokens(tokens[0]),\n",
    "    attention_head_names = [f\"{layer}.{head}\" for layer, head in top_heads],\n",
    ")\n",
    "\n",
    "path = \"attn_heads.html\"\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(str(attn_heads))\n",
    "\n",
    "webbrowser.open(path)\n",
    "```\n",
    "\n",
    "To check exactly where this is getting saved, you can print your current working directory with `os.getcwd()`.\n",
    "</details>\n",
    "\n",
    "From these plots, you might want to start thinking about the algorithm which is being implemented. In particular, for the attention heads with high positive attribution scores, where is `\" to\"` attending to? How might this head be affecting the logit diff score?\n",
    "\n",
    "We'll save a full hypothesis for how the model works until the end of the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmT44qAMtAgn"
   },
   "source": [
    "# 3Ô∏è‚É£ Activation Patching\n",
    "\n",
    "> ##### Learning Objectives\n",
    ">\n",
    "> * Understand the idea of activation patching, and how it can be used\n",
    ">     * Implement some of the activation patching helper functinos in transformerlens from scratch (i.e. using hooks)\n",
    "> * Use activation patching to track the layers & sequence positions in the residual stream where important information is stored and processed\n",
    "> * By the end of this section, you should be able to draw a rough sketch of the IOI circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD4i9RVBtAgn"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwgQsailtAgn"
   },
   "source": [
    "The obvious limitation to the techniques used above is that they only look at the very end of the circuit - the parts that directly affect the logits. Clearly this is not sufficient to understand the circuit! We want to understand how things compose together to produce this final output, and ideally to produce an end-to-end circuit fully explaining this behaviour.\n",
    "\n",
    "The technique we'll use to investigate this is called **activation patching**. This was first introduced in [David Bau and Kevin Meng's excellent ROME paper](https://rome.baulab.info/), there called causal tracing.\n",
    "\n",
    "The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then **intervene** on a specific activation and **patch** in the corresponding activation from the clean run (ie replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.\n",
    "\n",
    "We can then iterate over many possible activations and look at how much they affect the corrupted run. If patching in an activation significantly increases the probability of the correct answer, this allows us to *localise* which activations matter.\n",
    "\n",
    "The ability to localise is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what's going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBquILF3tAgn"
   },
   "source": [
    "The diagrams below demonstrate activation patching on an abstract neural network (the nodes represent activations, and the arrows between them are weight connections).\n",
    "\n",
    "A regular forward pass on the clean input looks like:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-1c.png\" width=\"300\">\n",
    "\n",
    "And activation patching from a corrupted input (green) into a forward pass for the clean input (black) looks like:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-2c.png\" width=\"440\">\n",
    "\n",
    "where the dotted line represents patching in a value (i.e. during the forward pass on the clean input, we replace node $D$ with the value it takes on the corrupted input). Nodes $H$, $G$ and $F$ are colored orange, to represent that they now follow a distribution which is not the same as clean or corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12uT6MiOtAgn"
   },
   "source": [
    "We can patch into a transformer in many different ways (e.g. values of the residual stream, the MLP, or attention heads' output - see below). We can also get even more granular by patching at particular sequence positions (not shown in diagram).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/simpler-patching-examples.png\" width=\"840\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox5WO6ZHtAgn"
   },
   "source": [
    "### Noising vs denoising\n",
    "\n",
    "We might call this algorithm a type of **noising**, since we're running the model on a clean input and adding noise by patching in from the corrupted input. We can also consider the opposite algorithm, **denoising**, where we run the model on a corrupted input and remove noise by patching in from the clean input.\n",
    "\n",
    "When would you use noising vs denoising? It depends on your goals. The results of denoising are much stronger, because showing that a component or set of components is sufficient for a task is a big deal. On the other hand, the complexity of transformers and interdependence of components means that noising a model can have unpredictable consequences. If loss goes up when we ablate a component, it doesn't necessarily mean that this component was necessary for the task. As an example, ablating MLP0 in gpt2-small seems to make performance much worse on basically any task (because it acts as a kind of extended embedding; more on this later in these exercises), but it's not doing anything important which is *specfic* for the IOI task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmCB8ALZtAgn"
   },
   "source": [
    "### Example: denoising the residual stream\n",
    "\n",
    "The above was all fairly abstract, so let's zoom in and lay out a concrete example to understand Indirect Object Identification. We'll start with an exercise on denoising, but we'll move onto noising later in this section.\n",
    "\n",
    "Here our clean input will be the original sentences (e.g. <code>\"When Mary and John went to the store, John gave a drink to\"</code>) and our corrupted input will have the subject token flipped (e.g. <code>\"When Mary and John went to the store, <u>Mary</u> gave a drink to\"</code>). Patching by replacing corrupted residual stream values with clean values is a causal intervention which will allow us to understand precisely which parts of the network are identifying the indirect object. If a component is important, then patching in (replacing that component's corrupted output with its clean output) will reverse the signal that this component produces, hence making performance much better.\n",
    "\n",
    "Note - the noising and denoising terminology doesn't exactly fit here, since the \"noised dataset\" actually **reverses** the signal rather than erasing it. The reason we're describing this as denoising is more a matter of framing - we're trying to figure out which components / activations are **sufficient** to recover performance, rather than which are **necessary**. If you're ever confused, this is a useful framing to have - **noising tells you what is necessary, denoising tells you what is sufficient.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  border-left: 6px solid #2e8b57;\n",
    "  background: linear-gradient(90deg, #f6fff8 0%, #f0fff4 100%);\n",
    "  padding: 1rem 1.25rem;\n",
    "  margin: 1.5rem 0;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n",
    "\">\n",
    "  <h2 style=\"\n",
    "    color: #2e8b57;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    letter-spacing: 0.5px;\n",
    "  \">AISAA Note</h2>\n",
    "  <p style=\"margin: 0; color: #2f3e46; line-height: 1.5;\">\n",
    "  It's worth thinking about how this contrasts with the zero ablation intervention we saw in the first half, which was an example of a <strong>noising</strong> algorithm\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb3rmWddtAgn"
   },
   "source": [
    "One natural thing to patch in is the residual stream at a specific layer and specific position. For example, the model is likely intitially doing some processing on the `S2` token to realise that it's a duplicate, but then uses attention to move that information to the `end` token. So patching in the residual stream at the `end` token will likely matter a lot in later layers but not at all in early layers.\n",
    "\n",
    "We can zoom in much further and patch in specific activations from specific layers. For example, we think that the output of head 9.9 on the final token is significant for directly connecting to the logits, so we predict that just patching the output of this head will significantly affect performance.\n",
    "\n",
    "Note that this technique does *not* tell us how the components of the circuit connect up, just what they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRJkr622tAgn"
   },
   "source": [
    "TransformerLens has helpful built-in functions to perform activation patching, but in order to understand the process better, you're now going to implement some of these functions from first principles (i.e. just using hooks). You'll be able to test your functions by comparing their output to the built-in functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNsJDA5HtAgn"
   },
   "outputs": [],
   "source": [
    "from transformer_lens import patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGvzKJfctAgn"
   },
   "source": [
    "## Creating a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RqdNHvPtAgo"
   },
   "source": [
    "Before we patch, we need to create a metric for evaluating a set of logits. Since we'll be running our **corrupted prompts** (with `S2` replaced with the wrong name) and patching in our **clean prompts**, it makes sense to choose a metric such that:\n",
    "\n",
    "* A value of zero means no change (from the performance on the corrupted prompt)\n",
    "* A value of one means clean performance has been completely recovered\n",
    "\n",
    "For example, if we patched in the entire clean prompt, we'd get a value of one. If our patching actually makes the model even better at solving the task than its regular behaviour on the clean prompt then we'd get a value greater than 1, but generally we expect values between 0 and 1.\n",
    "\n",
    "It also makes sense to have the metric be a linear function of the logit difference. This is enough to uniquely specify a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjvdZxDztAgo"
   },
   "outputs": [],
   "source": [
    "clean_tokens = tokens\n",
    "# Swap each adjacent pair to get corrupted tokens\n",
    "indices = [i + 1 if i % 2 == 0 else i - 1 for i in range(len(tokens))]\n",
    "corrupted_tokens = clean_tokens[indices]\n",
    "\n",
    "print(\n",
    "    \"Clean string 0:    \",\n",
    "    model.to_string(clean_tokens[0]),\n",
    "    \"\\nCorrupted string 0:\",\n",
    "    model.to_string(corrupted_tokens[0]),\n",
    ")\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = logits_to_ave_logit_diff(clean_logits, answer_tokens)\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhVuF-jEtAgo"
   },
   "source": [
    "### Exercise - create a metric\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "> Importance: üîµüîµüîµ‚ö™‚ö™\n",
    "> ```\n",
    "\n",
    "Fill in the function `ioi_metric` below, to create the required metric. Note that we can afford to use default arguments in this function, because we'll be using the same dataset for this whole section.\n",
    "\n",
    "**Important note** - this function needs to return a scalar tensor, rather than a float. If not, then some of the patching functions later on won't work. The type signature of this is `Float[Tensor, \"\"]`.\n",
    "\n",
    "**Second important note** - we've defined this to be 0 when performance is the same as on corrupted input, and 1 when it's the same as on clean input. This is because we're performing a **denoising algorithm**; we're looking for activations which are sufficient for recovering a model's performance (i.e. activations which have enough information to recover the correct answer from the corrupted input). Our \"null hypothesis\" is that the component isn't sufficient, and so patching it by replacing corrupted with clean values doesn't recover any performance. In later sections we'll be doing noising, and we'll define a new metric function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Int[Tensor, \"batch 2\"] = answer_tokens,\n",
    "    corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    Linear function of logit diff, calibrated so that it equals 0 when performance is same as on\n",
    "    corrupted input, and 1 when performance is same as on clean input.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "t.testing.assert_close(ioi_metric(clean_logits).item(), 1.0)\n",
    "t.testing.assert_close(ioi_metric(corrupted_logits).item(), 0.0)\n",
    "t.testing.assert_close(ioi_metric((clean_logits + corrupted_logits) / 2).item(), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDiWLKfltAgo"
   },
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Int[Tensor, \"batch 2\"] = answer_tokens,\n",
    "    corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    Linear function of logit diff, calibrated so that it equals 0 when performance is same as on\n",
    "    corrupted input, and 1 when performance is same as on clean input.\n",
    "    \"\"\"\n",
    "    patched_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)\n",
    "\n",
    "\n",
    "t.testing.assert_close(ioi_metric(clean_logits).item(), 1.0)\n",
    "t.testing.assert_close(ioi_metric(corrupted_logits).item(), 0.0)\n",
    "t.testing.assert_close(ioi_metric((clean_logits + corrupted_logits) / 2).item(), 0.5)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5nYU1VQtAgo"
   },
   "source": [
    "## Residual Stream Patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lGyn3oItAgo"
   },
   "source": [
    "Lets begin with a simple example: we patch in the residual stream at the start of each layer and for each token position. Before you write your own function to do this, let's see what this looks like with TransformerLens' `patching` module. Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQ4UUXOHtAgo"
   },
   "outputs": [],
   "source": [
    "act_patch_resid_pre = patching.get_act_patch_resid_pre(\n",
    "    model=model,\n",
    "    corrupted_tokens=corrupted_tokens,\n",
    "    clean_cache=clean_cache,\n",
    "    patching_metric=ioi_metric,\n",
    ")\n",
    "\n",
    "labels = [f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkYr97nstAgo"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_resid_pre,\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    "    x=labels,\n",
    "    title=\"resid_pre Activation Patching\",\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqOqspWvtAgo"
   },
   "source": [
    "Question - what is the interpretation of this graph? What significant things does it tell you about the nature of how the model solves this task?\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "Think about locality of computation.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "Originally all relevant computation happens on `S2`, and at layers 7 and 8, the information is moved to `END`. Moving the residual stream at the correct position near *exactly* recovers performance!\n",
    "\n",
    "To be clear, the striking thing about this graph isn't that the first row is zero everywhere except for `S2` where it is 1, or that the rows near the end trend to being zero everywhere except for `END` where they are 1; both of these are exactly what we'd expect. The striking things are:\n",
    "\n",
    "* The computation is highly localized; the relevant information for choosing `IO` over `S` is initially stored in `S2` token and then moved to `END` token without taking any detours.\n",
    "* The model is basically done after layer 8, and the rest of the layers actually slightly impede performance on this particular task.\n",
    "\n",
    "(Note - for reference, tokens and their index from the first prompt are on the x-axis. In an abuse of notation, note that the difference here is averaged over *all* 8 prompts, while the labels only come from the *first* prompt.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbrwyFBMtAgo"
   },
   "source": [
    "### Exercise - implement head-to-residual patching\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥üî¥üî¥‚ö™\n",
    "> Importance: üîµüîµüîµüîµüîµ\n",
    ">\n",
    "> It's very important to understand how patching works. Many subsequent exercises will build on this one.\n",
    "> ```\n",
    "\n",
    "Now, you should implement the `get_act_patch_resid_pre` function below, which should give you results just like the code you ran above. A quick refresher on how to use hooks in this way:\n",
    "\n",
    "* Hook functions take arguments `tensor: torch.Tensor` and `hook: HookPoint`. It's often easier to define a hook function taking more arguments than these, and then use `functools.partial` when it actually comes time to add your hook.\n",
    "* The function `model.run_with_hooks` takes arguments:\n",
    "    * The tokens to run (as first argument)\n",
    "    * `fwd_hooks` - a list of `(hook_name, hook_fn)` tuples. Remember that you can use `utils.get_act_name` to get hook names.\n",
    "* Tip - it's good practice to have `model.reset_hooks()` at the start of functions which add and run hooks. This is because sometimes hooks fail to be removed (if they cause an error while running). There's nothing more frustrating than fixing a hook error only to get the same error message, not realising that you've failed to clear the broken hook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_residual_component(\n",
    "    corrupted_residual_component: Float[Tensor, \"batch pos d_model\"],\n",
    "    hook: HookPoint,\n",
    "    pos: int,\n",
    "    clean_cache: ActivationCache,\n",
    ") -> Float[Tensor, \"batch pos d_model\"]:\n",
    "    \"\"\"\n",
    "    Patches a given sequence position in the residual stream, using the value\n",
    "    from the clean cache.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def get_act_patch_resid_pre(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable[[Float[Tensor, \"batch pos d_vocab\"]], float],\n",
    ") -> Float[Tensor, \"3 layer pos\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching each position at each layer in the residual\n",
    "    stream, using the value from the clean cache.\n",
    "\n",
    "    The results are calculated using the patching_metric function, which should be\n",
    "    called on the model's logit output.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "act_patch_resid_pre_own = get_act_patch_resid_pre(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "\n",
    "t.testing.assert_close(act_patch_resid_pre, act_patch_resid_pre_own)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JyKQwuutAgo"
   },
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def patch_residual_component(\n",
    "    corrupted_residual_component: Float[Tensor, \"batch pos d_model\"],\n",
    "    hook: HookPoint,\n",
    "    pos: int,\n",
    "    clean_cache: ActivationCache,\n",
    ") -> Float[Tensor, \"batch pos d_model\"]:\n",
    "    \"\"\"\n",
    "    Patches a given sequence position in the residual stream, using the value\n",
    "    from the clean cache.\n",
    "    \"\"\"\n",
    "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def get_act_patch_resid_pre(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable[[Float[Tensor, \"batch pos d_vocab\"]], float],\n",
    ") -> Float[Tensor, \"3 layer pos\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching each position at each layer in the residual\n",
    "    stream, using the value from the clean cache.\n",
    "\n",
    "    The results are calculated using the patching_metric function, which should be\n",
    "    called on the model's logit output.\n",
    "    \"\"\"\n",
    "    model.reset_hooks()\n",
    "    seq_len = corrupted_tokens.size(1)\n",
    "    results = t.zeros(model.cfg.n_layers, seq_len, device=device, dtype=t.float32)\n",
    "\n",
    "    for layer in tqdm(range(model.cfg.n_layers)):\n",
    "        for position in range(seq_len):\n",
    "            hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)\n",
    "            patched_logits = model.run_with_hooks(\n",
    "                corrupted_tokens,\n",
    "                fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), hook_fn)],\n",
    "            )\n",
    "            results[layer, position] = patching_metric(patched_logits)\n",
    "\n",
    "    return results\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qm1wKdcztAgo"
   },
   "source": [
    "Once you've passed the tests, you can plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAJIvfDQtAgo"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_resid_pre_own,\n",
    "    x=labels,\n",
    "    title=\"Logit Difference From Patched Residual Stream\",\n",
    "    labels={\"x\": \"Sequence Position\", \"y\": \"Layer\"},\n",
    "    width=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXK5MejjtAgo"
   },
   "source": [
    "## Patching in residual stream by block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7abA8e2tAgo"
   },
   "source": [
    "Rather than just patching to the residual stream in each layer, we can also patch just after the attention layer or just after the MLP. This gives is a slightly more refined view of which tokens matter and when.\n",
    "\n",
    "The function `patching.get_act_patch_block_every` works just like `get_act_patch_resid_pre`, but rather than just patching to the residual stream, it patches to `resid_pre`, `attn_out` and `mlp_out`, and returns a tensor of shape `(3, n_layers, seq_len)`.\n",
    "\n",
    "One important thing to note - we're cycling through the `resid_pre`, `attn_out` and `mlp_out` and only patching one of them at a time, rather than patching all three at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbCBq7rxtAgo"
   },
   "outputs": [],
   "source": [
    "act_patch_block_every = patching.get_act_patch_block_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fngz1q96tAgo"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_block_every,\n",
    "    x=labels,\n",
    "    facet_col=0, # This argument tells plotly which dimension to split into separate plots\n",
    "    facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"], # Subtitles of separate plots\n",
    "    title=\"Logit Difference From Patched Attn Head Output\",\n",
    "    labels={\"x\": \"Sequence Position\", \"y\": \"Layer\"},\n",
    "    width=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-19d0pThtAgo"
   },
   "source": [
    "<details>\n",
    "<summary>Question - what is the interpretation of the second two plots?</summary>\n",
    "\n",
    "We see that several attention layers are significant but that, matching the residual stream results, early layers matter on `S2`, and later layers matter on `END`, and layers essentially don't matter on any other token. Extremely localised!\n",
    "\n",
    "As with direct logit attribution, layer 9 is positive and layers 10 and 11 are not, suggesting that the late layers only matter for direct logit effects, but we also see that layers 7 and 8 matter significantly. Presumably these are the heads that move information about which name is duplicated from `S2` to `END`.\n",
    "\n",
    "In contrast, the MLP layers do not matter much. This makes sense, since this is more a task about moving information than about processing it, and the MLP layers specialise in processing information. The one exception is MLP0, which matters a lot, but I think this is misleading and just a generally true statement about MLP0 rather than being about the circuit on this task. Read won for an interesting aside about MLP0!\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>An aside on knowledge storage in the MLP</summary>\n",
    "\n",
    "We may have mentioned at some point that \"facts\" or \"knowledge\" are stored in the MLP layers.\n",
    "Here's an example using our previous function to investigate this claim:\n",
    "Given the prompt `The White House is where the`, we would expect that gpt2 would guess ` president` as the answer (as part of the completion `The White House is where the president lives.`)\n",
    "and given the prompt `The Haunted House is where the`, we would expect that gpt2 would guess `ghosts` as the answer (as part of the completion `The Haunted House is where the ghosts live.`)\n",
    "\n",
    "Indeed this is the case (mostly because I cherry-picked these prompts to get a clean example where I can swap out a single-token word, and get a different single token answer).\n",
    "How does the model do this? Somewhere it has to have the association between White House/President and Haunted House/Ghosts.\n",
    "\n",
    "We can see this by feeding the two prompts through, and using as our metric the logit difference between the tokens ` president` and ` ghosts`.\n",
    "\n",
    "```python\n",
    "clean_prompt, clean_answer = \"The White House is where the\", \" president\" #Note the space in the answer!\n",
    "corrupted_prompt, corrupted_answer = \"The Haunted House is where the\", \" ghosts\"\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompt)\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
    "\n",
    "assert clean_tokens.shape == corrupted_tokens.shape, \"clean and corrupted tokens must have same shape\"\n",
    "\n",
    "clean_token = model.to_single_token(clean_answer)\n",
    "corrupted_token = model.to_single_token(corrupted_answer)\n",
    "\n",
    "utils.test_prompt(clean_prompt, clean_answer, model)\n",
    "utils.test_prompt(corrupted_prompt, corrupted_answer, model)\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "\n",
    "def answer_metric(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    clean_token: Int = clean_token,\n",
    "    corrupted_token: Int = corrupted_token,\n",
    ") -> Float[Tensor, \"batch\"]:\n",
    "    return logits[:, -1, clean_token] - logits[:, -1, corrupted_token]\n",
    "\n",
    "act_patch_block_every = patching.get_act_patch_block_every(model, corrupted_tokens, clean_cache, answer_metric)\n",
    "\n",
    "imshow(\n",
    "    act_patch_block_every,\n",
    "    x=[\"<endoftext>\",\"The\", \"White/Haunted\", \"House\", \"is\", \"where\", \"the\"],\n",
    "    facet_col=0,  # This argument tells plotly which dimension to split into separate plots\n",
    "    facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"],  # Subtitles of separate plots\n",
    "    title=\"Logit Difference (president - ghosts)\",\n",
    "    labels={\"x\": \"Sequence Position\", \"y\": \"Layer\"},\n",
    "    width=1200,\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V9KJMlstAgo"
   },
   "source": [
    "### Exercise - implement head-to-block patching\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "> Importance: üîµüîµ‚ö™‚ö™‚ö™\n",
    ">\n",
    "> Most code can be copied from the last exercise.\n",
    "> ```\n",
    "\n",
    "If you want, you can implement the `get_act_patch_resid_pre` function for fun, although it's similar enough to the previous exercise that doing this isn't compulsory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_patch_block_every(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable[[Float[Tensor, \"batch pos d_vocab\"]], float],\n",
    ") -> Float[Tensor, \"3 layer pos\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching each position at each layer in the residual stream,\n",
    "    using the value from the clean cache.\n",
    "\n",
    "    The results are calculated using the patching_metric function, which should be called on the\n",
    "    model's logit output.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "act_patch_block_every_own = get_act_patch_block_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "\n",
    "t.testing.assert_close(act_patch_block_every, act_patch_block_every_own)\n",
    "\n",
    "imshow(\n",
    "    act_patch_block_every_own,\n",
    "    x=labels,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"],\n",
    "    title=\"Logit Difference From Patched Attn Head Output\",\n",
    "    labels={\"x\": \"Sequence Position\", \"y\": \"Layer\"},\n",
    "    width=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTpLQfY9tAgo"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_block_every_own,\n",
    "    x=labels,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"],\n",
    "    title=\"Logit Difference From Patched Attn Head Output\",\n",
    "    labels={\"x\": \"Sequence Position\", \"y\": \"Layer\"},\n",
    "    width=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxqH26RGtAgo"
   },
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def get_act_patch_block_every(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable[[Float[Tensor, \"batch pos d_vocab\"]], float],\n",
    ") -> Float[Tensor, \"3 layer pos\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching each position at each layer in the residual stream,\n",
    "    using the value from the clean cache.\n",
    "\n",
    "    The results are calculated using the patching_metric function, which should be called on the\n",
    "    model's logit output.\n",
    "    \"\"\"\n",
    "    model.reset_hooks()\n",
    "    results = t.zeros(3, model.cfg.n_layers, tokens.size(1), device=device, dtype=t.float32)\n",
    "\n",
    "    for component_idx, component in enumerate([\"resid_pre\", \"attn_out\", \"mlp_out\"]):\n",
    "        for layer in tqdm(range(model.cfg.n_layers)):\n",
    "            for position in range(corrupted_tokens.shape[1]):\n",
    "                hook_fn = partial(patch_residual_component, pos=position, clean_cache=clean_cache)\n",
    "                patched_logits = model.run_with_hooks(\n",
    "                    corrupted_tokens,\n",
    "                    fwd_hooks=[(utils.get_act_name(component, layer), hook_fn)],\n",
    "                )\n",
    "                results[component_idx, layer, position] = patching_metric(patched_logits)\n",
    "\n",
    "    return results\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9SlMhSOtAgo"
   },
   "source": [
    "## Head Patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82TqsgmftAgo"
   },
   "source": [
    "We can refine the above analysis by patching in individual heads! This is somewhat more annoying, because there are now three dimensions `(head_index, position and layer)`.\n",
    "\n",
    "The code below patches a head's output over all sequence positions, and returns the results (for each head in the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLzeIa4mtAgo"
   },
   "outputs": [],
   "source": [
    "act_patch_attn_head_out_all_pos = patching.get_act_patch_attn_head_out_all_pos(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCE3bNlltAgp"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_attn_head_out_all_pos,\n",
    "    labels={\"y\": \"Layer\", \"x\": \"Head\"},\n",
    "    title=\"attn_head_out Activation Patching (All Pos)\",\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBsOIvlhtAgp"
   },
   "source": [
    "<details>\n",
    "<summary>Question - what are the interpretations of this graph? Which heads do you think are important?</summary>\n",
    "\n",
    "We see some of the heads that we observed in our attention plots at the end of last section (e.g. `9.9` having a large positive score, and `10.7` having a large negative score). But we can also see some other important heads, for instance:\n",
    "\n",
    "* In layers 7-8 there are several important heads. We might deduce that these are the ones responsible for moving information from `S2` to `end`.\n",
    "* In the earlier layers, there are some more important heads (e.g. `3.0` and `5.5`). We might guess these are performing some primitive logic, e.g. causing the second `\" John\"` token to attend to previous instances of itself.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXUr70FltAgp"
   },
   "source": [
    "### Exercise - implement head-to-head patching\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥üî¥‚ö™‚ö™\n",
    "> Importance: üîµüîµüîµüîµ‚ö™\n",
    ">\n",
    "> Again, it should be similar to the first patching exercise (you can copy code).\n",
    "> ```\n",
    "\n",
    "You should implement your own version of this patching function below.\n",
    "\n",
    "You'll need to define a new hook function, but most of the code from the previous exercise should be reusable.\n",
    "\n",
    "<details>\n",
    "<summary>Help - I'm not sure what hook name to use for my patching.</summary>\n",
    "\n",
    "You should patch at:\n",
    "\n",
    "```python\n",
    "utils.get_act_name(\"z\", layer)\n",
    "```\n",
    "\n",
    "This is the linear combination of value vectors, i.e. it's the thing you multiply by $W_O$ before adding back into the residual stream. There's no point patching after the $W_O$ multiplication, because it will have the same effect, but take up more memory (since `d_model` is larger than `d_head`).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cBzUwQmtAgp"
   },
   "outputs": [],
   "source": [
    "def patch_head_vector(\n",
    "    corrupted_head_vector: Float[Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    head_index: int,\n",
    "    clean_cache: ActivationCache,\n",
    ") -> Float[Tensor, \"batch pos head_index d_head\"]:\n",
    "    \"\"\"\n",
    "    Patches the output of a given head (before it's added to the residual stream) at every sequence\n",
    "    position, using the value from the clean cache.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def get_act_patch_attn_head_out_all_pos(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable,\n",
    ") -> Float[Tensor, \"layer head\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching at all positions for each head in each layer, using the\n",
    "    value from the clean cache. The results are calculated using the patching_metric function, which\n",
    "    should be called on the model's logit output.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMkRdJ64tAgp"
   },
   "outputs": [],
   "source": [
    "act_patch_attn_head_out_all_pos_own = get_act_patch_attn_head_out_all_pos(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "\n",
    "t.testing.assert_close(act_patch_attn_head_out_all_pos, act_patch_attn_head_out_all_pos_own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsddlMhLtAgp"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_attn_head_out_all_pos_own,\n",
    "    title=\"Logit Difference From Patched Attn Head Output\",\n",
    "    labels={\"x\":\"Head\", \"y\":\"Layer\"},\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bm5Hg54CtAgp"
   },
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def patch_head_vector(\n",
    "    corrupted_head_vector: Float[Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    head_index: int,\n",
    "    clean_cache: ActivationCache,\n",
    ") -> Float[Tensor, \"batch pos head_index d_head\"]:\n",
    "    \"\"\"\n",
    "    Patches the output of a given head (before it's added to the residual stream) at every sequence\n",
    "    position, using the value from the clean cache.\n",
    "    \"\"\"\n",
    "    corrupted_head_vector[:, :, head_index] = clean_cache[hook.name][:, :, head_index]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "def get_act_patch_attn_head_out_all_pos(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable,\n",
    ") -> Float[Tensor, \"layer head\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching at all positions for each head in each layer, using the\n",
    "    value from the clean cache. The results are calculated using the patching_metric function, which\n",
    "    should be called on the model's logit output.\n",
    "    \"\"\"\n",
    "    model.reset_hooks()\n",
    "    results = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)\n",
    "\n",
    "    for layer in tqdm(range(model.cfg.n_layers)):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            hook_fn = partial(patch_head_vector, head_index=head, clean_cache=clean_cache)\n",
    "            patched_logits = model.run_with_hooks(\n",
    "                corrupted_tokens,\n",
    "                fwd_hooks=[(utils.get_act_name(\"z\", layer), hook_fn)],\n",
    "                return_type=\"logits\",\n",
    "            )\n",
    "            results[layer, head] = patching_metric(patched_logits)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "act_patch_attn_head_out_all_pos_own = get_act_patch_attn_head_out_all_pos(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "\n",
    "t.testing.assert_close(act_patch_attn_head_out_all_pos, act_patch_attn_head_out_all_pos_own)\n",
    "\n",
    "imshow(\n",
    "    act_patch_attn_head_out_all_pos_own,\n",
    "    title=\"Logit Difference From Patched Attn Head Output\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    width=600,\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3AG7sw5tAgp"
   },
   "source": [
    "## Decomposing Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5etIHlmatAgp"
   },
   "source": [
    "Finally, we'll look at one more example of activation patching.\n",
    "\n",
    "Decomposing attention layers into patching in individual heads has already helped us localise the behaviour a lot. But we can understand it further by decomposing heads. An attention head consists of two semi-independent operations - calculating *where* to move information from and to (represented by the attention pattern and implemented via the QK-circuit) and calculating *what* information to move (represented by the value vectors and implemented by the OV circuit). We can disentangle which of these is important by patching in just the attention pattern *or* the value vectors. See [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) or [Neel's walkthrough video](https://www.youtube.com/watch?v=KV5gbOmHbjU) for more on this decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIRRJURPtAgp"
   },
   "source": [
    "A useful function for doing this is `get_act_patch_attn_head_all_pos_every`. Rather than just patching on head output (like the previous one), it patches on:\n",
    "* Output (this is equivalent to patching the value the head writes to the residual stream)\n",
    "* Querys (i.e. the patching the query vectors, without changing the key or value vectors)\n",
    "* Keys\n",
    "* Values\n",
    "* Patterns (i.e. the attention patterns).\n",
    "\n",
    "Again, note that this function isn't patching multiple things at once. It's looping through each of these five, and getting the results from patching them one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-Y55rJCtAgp"
   },
   "outputs": [],
   "source": [
    "act_patch_attn_head_all_pos_every = patching.get_act_patch_attn_head_all_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZU25mIntAgp"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_attn_head_all_pos_every,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
    "    title=\"Activation Patching Per Head (All Pos)\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FzxzmC7tAgp"
   },
   "source": [
    "### Exercise (optional) - implement head-to-head-input patching\n",
    "\n",
    "> ```yaml\n",
    "> Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
    "> Importance: üîµüîµ‚ö™‚ö™‚ö™\n",
    ">\n",
    "> Most code can be copied from the last exercise.\n",
    "> ```\n",
    "\n",
    "Again, if you want to implement this yourself then you can do so below, but it isn't a compulsory exercise because it isn't conceptually different from the previous exercises. If you don't implement it, then you should still look at the solution to make sure you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm5I-U6xtAgp"
   },
   "outputs": [],
   "source": [
    "def patch_attn_patterns(\n",
    "    corrupted_head_vector: Float[Tensor, \"batch head_index pos_q pos_k\"],\n",
    "    hook: HookPoint,\n",
    "    head_index: int,\n",
    "    clean_cache: ActivationCache,\n",
    ") -> Float[Tensor, \"batch pos head_index d_head\"]:\n",
    "    \"\"\"\n",
    "    Patches the attn patterns of a given head at every sequence position, using the value from the\n",
    "    clean cache.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def get_act_patch_attn_head_all_pos_every(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable,\n",
    ") -> Float[Tensor, \"layer head\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching at all positions for each head in each layer (using the\n",
    "    value from the clean cache) for output, queries, keys, values and attn pattern in turn.\n",
    "\n",
    "    The results are calculated using the patching_metric function, which should be called on the\n",
    "    model's logit output.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "act_patch_attn_head_all_pos_every_own = get_act_patch_attn_head_all_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "\n",
    "t.testing.assert_close(act_patch_attn_head_all_pos_every, act_patch_attn_head_all_pos_every_own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN382zxDtAgp"
   },
   "outputs": [],
   "source": [
    "imshow(\n",
    "    act_patch_attn_head_all_pos_every_own,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
    "    title=\"Activation Patching Per Head (All Pos)\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    width=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bm-fy6yXtAgp"
   },
   "source": [
    "<details><summary>Solution</summary>\n",
    "\n",
    "```python\n",
    "def patch_attn_patterns(\n",
    "    corrupted_head_vector: Float[Tensor, \"batch head_index pos_q pos_k\"],\n",
    "    hook: HookPoint,\n",
    "    head_index: int,\n",
    "    clean_cache: ActivationCache,\n",
    ") -> Float[Tensor, \"batch pos head_index d_head\"]:\n",
    "    \"\"\"\n",
    "    Patches the attn patterns of a given head at every sequence position, using the value from the\n",
    "    clean cache.\n",
    "    \"\"\"\n",
    "    corrupted_head_vector[:, head_index] = clean_cache[hook.name][:, head_index]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "def get_act_patch_attn_head_all_pos_every(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[Tensor, \"batch pos\"],\n",
    "    clean_cache: ActivationCache,\n",
    "    patching_metric: Callable,\n",
    ") -> Float[Tensor, \"layer head\"]:\n",
    "    \"\"\"\n",
    "    Returns an array of results of patching at all positions for each head in each layer (using the\n",
    "    value from the clean cache) for output, queries, keys, values and attn pattern in turn.\n",
    "\n",
    "    The results are calculated using the patching_metric function, which should be called on the\n",
    "    model's logit output.\n",
    "    \"\"\"\n",
    "    results = t.zeros(5, model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)\n",
    "    # Loop over each component in turn\n",
    "    for component_idx, component in enumerate([\"z\", \"q\", \"k\", \"v\", \"pattern\"]):\n",
    "        for layer in tqdm(range(model.cfg.n_layers)):\n",
    "            for head in range(model.cfg.n_heads):\n",
    "                # Get different hook function if we're doing attention probs\n",
    "                hook_fn_general = (\n",
    "                    patch_attn_patterns if component == \"pattern\" else patch_head_vector\n",
    "                )\n",
    "                hook_fn = partial(hook_fn_general, head_index=head, clean_cache=clean_cache)\n",
    "                # Get patched logits\n",
    "                patched_logits = model.run_with_hooks(\n",
    "                    corrupted_tokens,\n",
    "                    fwd_hooks=[(utils.get_act_name(component, layer), hook_fn)],\n",
    "                    return_type=\"logits\",\n",
    "                )\n",
    "                results[component_idx, layer, head] = patching_metric(patched_logits)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "act_patch_attn_head_all_pos_every_own = get_act_patch_attn_head_all_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "\n",
    "t.testing.assert_close(act_patch_attn_head_all_pos_every, act_patch_attn_head_all_pos_every_own)\n",
    "\n",
    "imshow(\n",
    "    act_patch_attn_head_all_pos_every_own,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
    "    title=\"Activation Patching Per Head (All Pos)\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    width=1200,\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqE4LR6GtAgp"
   },
   "source": [
    "Note - we can do this in an even more fine-grained way; the function `patching.get_act_patch_attn_head_by_pos_every` (i.e. same as above but replacing `all_pos` with `by_pos`) will give you the same decomposition, but by sequence position *as well as* by layer, head and component. The same holds for the `patching.get_act_patch_attn_head_out_all_pos` function earlier (replace `all_pos` with `by_pos`). These functions are unsurprisingly pretty slow though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8qHOAp-tAgp"
   },
   "source": [
    "This plot has some striking features. For instance, this shows us that we have at least three different groups of heads:\n",
    "\n",
    "* Earlier heads (`3.0`, `5.5`, `6.9`) which matter because of their attention patterns (specifically their query vectors).\n",
    "* Middle heads in layers 7 & 8 (`7.3`, `7.9`, `8.6`, `8.10`) seem to matter more because of their value vectors.\n",
    "* Later heads which improve the logit difference (`9.9`, `10.0`), which matter because of their query vectors.\n",
    "\n",
    "Question - what is the significance of the results for the middle heads (i.e. the important ones in layers 7 & 8)? In particular, how should we interpret the fact that value patching has a much bigger effect than the other two forms of patching?\n",
    "\n",
    "*Hint - if you're confused, try plotting the attention patterns of heads `7.3`, `7.9`, `8.6`, `8.10`. You can mostly reuse the code from above when we displayed the output of attention heads.*\n",
    "\n",
    "<details>\n",
    "<summary>Code to plot attention heads</summary>\n",
    "\n",
    "```python\n",
    "# Get the heads with largest value patching\n",
    "# (we know from plot above that these are the 4 heads in layers 7 & 8)\n",
    "k = 4\n",
    "top_heads = topk_of_Nd_tensor(act_patch_attn_head_all_pos_every[3], k=k)\n",
    "\n",
    "# Get all their attention patterns\n",
    "attn_patterns_for_important_heads: Float[Tensor, \"head q k\"] = t.stack([\n",
    "    cache[\"pattern\", layer][:, head].mean(0)\n",
    "        for layer, head in top_heads\n",
    "])\n",
    "\n",
    "# Display results\n",
    "display(HTML(f\"<h2>Top {k} Logit Attribution Heads (from value-patching)</h2>\"))\n",
    "display(cv.attention.attention_patterns(\n",
    "    attention = attn_patterns_for_important_heads,\n",
    "    tokens = model.to_str_tokens(tokens[0]),\n",
    "    attention_head_names = [f\"{layer}.{head}\" for layer, head in top_heads],\n",
    "))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "The attention patterns show us that these heads attend from `END` to `S2`, so we can guess that they're responsible for moving information from `S2` to `END` which is used to determine the answer. This agrees with our earlier results, when we saw that most of the information gets moved over layers 7 & 8.\n",
    "\n",
    "The fact that value patching is the most important thing for them suggests that the interesting computation goes into **what information they move from `S2` to `end`**, rather than **why `end` attends to `S2`**. See the diagram below if you're confused why we can draw this inference.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/k-vs-v-patching-explained.png\" width=\"900\">\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC2oqstctAgp"
   },
   "source": [
    "## Consolidating Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1-0XIcktAgp"
   },
   "source": [
    "OK, let's zoom out and reconsolidate. Here's a recap of the most important observations we have so far:\n",
    "\n",
    "* Heads `9.9`, `9.6`, and `10.0` are the most important heads in terms of directly writing to the residual stream. In all these heads, the `END` attends strongly to the `IO`.\n",
    "    * We discovered this by taking the values written by each head in each layer to the residual stream, and projecting them along the logit diff direction by using `residual_stack_to_logit_diff`. We also looked at attention patterns using `circuitsvis`.\n",
    "    * <span style=\"color:darkorange\">**This suggests that these heads are copying `IO` to `end`, to use it as the predicted next token.**</span>\n",
    "    * The question then becomes *\"how do these heads know to attend to this token, and not attend to `S`?\"*\n",
    "\n",
    "<br>\n",
    "\n",
    "* All the action is on `S2` until layer 7 and then transitions to `END`. And that attention layers matter a lot, MLP layers not so much (apart from MLP0, likely as an extended embedding).\n",
    "    * We discovered this by doing **activation patching** on `resid_pre`, `attn_out`, and `mlp_out`.\n",
    "    * <span style=\"color:darkorange\">**This suggests that there is a cluster of heads in layers 7 & 8, which move information from `S2` to `END`. We deduce that this information is how heads `9.9`, `9.6` and `10.0` know to attend to `IO`.**</span>\n",
    "    * The question then becomes *\"what is this information, how does it end up in the `S2` token, and how does `END` know to attend to it?\"*\n",
    "\n",
    "<br>\n",
    "\n",
    "* The significant heads in layers 7 & 8 are `7.3`, `7.9`, `8.6`, `8.10`. These heads have high activation patching values for their value vectors, less so for their queries and keys.\n",
    "    * We discovered this by doing **activation patching** on the value inputs for these heads.\n",
    "    * <span style=\"color:darkorange\">**This supports the previous observation, and it tells us that the interesting computation goes into *what gets moved* from `S2` to `END`, rather than the fact that `END` attends to `S2`.**</span>.\n",
    "    * We still don't know: *\"what is this information, and how does it end up in the `S2` token?\"*\n",
    "\n",
    "<br>\n",
    "\n",
    "* As well as the 2 clusters of heads given above, there's a third cluster of important heads: early heads (e.g. `3.0`, `5.5`, `6.9`) whose query vectors are particularly important for getting good performance.\n",
    "    * We discovered this by doing **activation patching** on the query inputs for these heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  border-left: 6px solid #2e8b57;\n",
    "  background: linear-gradient(90deg, #f6fff8 0%, #f0fff4 100%);\n",
    "  padding: 1rem 1.25rem;\n",
    "  margin: 1.5rem 0;\n",
    "  border-radius: 10px;\n",
    "  box-shadow: 0 2px 6px rgba(0,0,0,0.05);\n",
    "\">\n",
    "  <h2 style=\"\n",
    "    color: #2e8b57;\n",
    "    margin-top: 0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    letter-spacing: 0.5px;\n",
    "  \">AISAA Note: Conclusion</h2>\n",
    "  <p style=\"margin: 0; color: #2f3e46; line-height: 1.5;\">\n",
    "    To recap, we've seen a variety of tools to help us understand which layers are most important for the task of indirect object identification.\n",
    "\n",
    "These included:\n",
    "1. **Zero-ablation**: observing how the model output changes (if at all), once we remove its activations -- are these activations necessary for the task? This was a kind of **noising** algorithm. We're starting from the clean run, and patching in a form of corrupted activations.\n",
    "2. **Direct logit attribution**: looking at the direct influence of a layer's output to the residual stream in increasing the logits for the correct token\n",
    "3. **Activation patching**: patching in target layers' activations from a 'clean' run onto a 'corrupted' run; are these activations sufficient to modify the output to the correct answer?\n",
    "\n",
    "Hopefully along the way we've learned about the benefits of the `TransformerLens` library for efficiently collecting a load of activations for downstream interpretability tasks!\n",
    "\n",
    "## Going further \n",
    "\n",
    "The rest of the ARENA tutorial uses the tools here to connect to the concrete results in the [original paper](https://arxiv.org/abs/2211.00593). They also introduce the even more sophisticated tool of **path patching**. You can check out the full version here:\n",
    "* [https://arena-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification](https://arena-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification)\n",
    "  </p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
